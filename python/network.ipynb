{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Docking Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "## !pip install -e ../gym_space_docking/\n",
    "\n",
    "loc = os.popen('pip3 show gym_space_docking').readlines()[7].split()[1]\n",
    "\n",
    "\n",
    "sys.path.append(loc)\n",
    "\n",
    "\n",
    "import gym\n",
    "import os\n",
    "import gym_space_docking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include Network\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create enviroment pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.9.6)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "/Users/denny/Documents/workspace_ironhack/IronHack_Final_Project/gym_space_docking/gym_space_docking/envs\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('space_docking-v0')\n",
    "#print(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(7)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "#print(env.observation_space)\n",
    "#print(type(env.action_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    map, reward, done, info = env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD7CAYAAACR4IPAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPhklEQVR4nO3df5BddXnH8fenGyK/ykDMKinJdomDKJOBoHcQDdOhYDrBksRxRgsztGqd0am2TTsySPij/GXNdDKO/tEyzQCKI0XSGEZgGDSjosV2MuxKqYGwgCtu0gQSLLRWOmrw6R/33M1h9+7uuT/Ovd9z7+c1k9lzv3s353vCPjzPPed7nqOIwMzS81v9noCZNefgNEuUg9MsUQ5Os0Q5OM0S5eA0S1RHwSlpk6QpSc9JurlbkzIzULvXOSWNAM8AG4HDwGPA9RHxVPemZza8lnXws5cBz0XENICkrwFbgQWDc+XKlTE+Pt7BLs0Gz+Tk5EsRMTp3vJPgPA84lHt9GHjXYj8wPj7OxMREB7s0GzySftpsvJPPnGoyNq9GlvRxSROSJo4fP97B7syGSyfBeRhYk3u9Gjgy900RsSsiahFRGx2dl7nNbAGdBOdjwAWSzpe0HLgOuL870zKztj9zRsQJSX8OfBMYAe6MiCe7NjOzIdfJCSEi4iHgoS7NxcxyvELILFEOTrNEOTjNEuXgNEuUg9MsUQ5Os0Q5OM0S5eA0S1RHixDMUjM2Nja7PTMz08eZdM6Z0yxRzpw2UIpmyypkWGdOs0Q5OM0S5bLWhlKqpWyeM6dZopw5baBU4URPUUtmTkl3Sjom6UBubIWkfZKezb6eU+40zYZPkbL2y8CmOWM3A9+OiAuAb2evzayLlixrI+L7ksbnDG8Frsy27wIeAT7TzYmZLaRRujYrW7tdyvazTG73hNCbI+IoQPb1TQu90X1rzdpT+gmhiNgF7AKo1WrtPZjFLKfVVUCdZLx+nlRqN3O+KGkVQPb1WPemZGbQfnDeD3w42/4w8I3uTMfMGpYsayXdQ/3kz0pJh4FbgR3AbkkfA2aAD5Y5Sau2K664Ynb70Ucf7dl+q36ds8jZ2usX+NbVXZ6LmeV4hZCVrqxs2a+M3CteW2uWKAenWaJc1lplDWIpm+fMaZYoZ05LxiDd7tUNzpxmiXJwmiXKZW0XvPbJ0xb9/t89cWLe2PYf/Lqs6VhJurGQvhXOnGaJcuZs0RNPPDFvbOaa3bPbB3Z8YMGfveVfT2bQ7d2dVul6cbKmk7+3F1mtKjdbm1nJHJxmiXJZW1CjnG1W2mzZsmV2+2/fs/A/6WLfS13q1x1Tn187nDnNElXd/5X3SOMyyUPZ5ZB1N++d954qZ8RhUNWVR0WaSq+R9F1JByU9KWlbNu7G0mYlKlLWngA+HRFvBy4HPiXpItxY2qxURdqUHAUaPWp/LukgcB4D3Fg6v+KnsbqncY3y/tz7FrumOQyWKheLXnvs9cqb/D4X2m8nc+rW8bR0Qijr/H4psJ+CjaXdVNqsPYoo1udZ0pnA94DPRsReSa9ExNm5778cEYt+7qzVajExMdHJfEvVyJjN1sK2Kn/i6Jl//CQAv3r5hdkxr62db6meQP3IsL0gaTIianPHC2VOSacAXwfujojGb50bS5uVqEjfWgF3AAcj4vO5bzUaS++gwo2lm32+vPeZ12bH/uitI697/9F3/tns9qrJ24Dml1fy3vqJfwCq+Rm1G9mq6KWMVtuOrM1tT7e4ryoocoFuA/DHwI8k/Xs2dgtuLG1WqiJnax8FtMC33VjarCRe2tLE3FI2r1HKApy+5qJeTKev+lUaFimnp5uMVb2UzfPaWrNEDW3m7MZlk1cPPQXAj79y8tbpt/zJ5+a9r4ongrqpW9lskLJiEc6cZolycJolamjL2m76vyNTs9uvHn0OgOm7burXdAZCo4Rtdt2yKiuFOr3m6sxplihnzibyK35aPZnjjNldzTJO6hmzoad3pZhZ7zg4zRI1tGXtQ1kj6AvedXR27A0rVs1732KL2of9+mWvtXoiqConjhbizGmWqKHNnA3NsmVRjazqDNobrWbAbrcf6TVnTrNEOTjNErVkDyFJpwLfB95AvQzeExG3SloB3AuMA88DH4qIlxf7u1LvIWTdVaUSsp866SH0S+CqiLgEWA9sknQ57ltrVqoinRAC+N/s5SnZn2CA+9ZadwxDxiyzOijafW8k6x90DNgXEYX71ppZewoFZ0S8FhHrgdXAZZLWFd2Bm0qbtaels7UR8Qr18nUTBfvWRsSuiKhFRG10dLSz2VoSxsbGXnc71CAqeowzMzOlle9FnjI2KunsbPs04L3A05zsWwsV7ltrlqoiK4RWAXdJGqEezLsj4kFJ/4b71g6lYTjRk8IxFjlb+x/UH140d/xnuG+tWWm8QsgsUQ5O67thOMHUDgenWaKG/pYx679WT74My5pdZ06zRDk4zRLlstYqp9vlbKplsjOnWaKcOW3opZYxG5w5zRLl4DRLlIPTLFEOTrNEOTjNEuXgNEuUg9MsUYWDM+vA97ikB7PXKyTtk/Rs9vWc8qZpNnxayZzbgIO5124qbVaion1rVwN/CNyeG95KvZk02df3d3VmZkOuaOb8AnAT8JvcWKGm0u5ba9aeIq0xrwWORcRkOztw31qz9hRZ+L4B2CLpfcCpwFmSvkrWVDoiji7WVNrM2rNk5oyI7RGxOiLGgeuA70TEDbiptFmpOrnOuQPYKOlZYGP22sy6pKX7OSPiEerPSnFTabOSeYWQWaIcnGaJcnCaJcrBaZYoB6dZohycZolycJolysFpligHp1miHJxmiXJwmiXKwWmWKAenWaIcnGaJcnCaJcrBaZaoQjdbS3oe+DnwGnAiImqSVgD3AuPA88CHIuLlcqZpNnxayZy/HxHrI6KWvXZTabMSdVLWuqm0WYmKBmcA35I0Kenj2ZibSpuVqGiDrw0RcUTSm4B9kp4uuoOI2AXsAqjVatHGHM2GUqHMGRFHsq/HgPuAy8iaSgO4qbRZ9xV5HMMZkn67sQ38AXAAN5U2K1WRsvbNwH2SGu//p4h4WNJjwG5JHwNmgA+WN02z4bNkcEbENHBJk3E3lTYrkVcImSXKwWmWKAenWaIcnGaJcnCaJcrBaZYoB6dZohycZolycJolysFpligHp1miHJxmiXJwmiXKwWmWKAenWaKK9q09G7gdWEe92defAlNUrG/t2NjY7PbMzEwfZ2K2tKKZ84vAwxHxNuo3Xh/EfWvNSlWkh9BZwO8BdwBExK8i4hXct9asVEXK2rXAceBLki4BJoFtzOlbm7XNTFrRUvaBBx6Y3T6w4wMt7WPdzXsB2Lx5c0s/ZzZXkbJ2GfAO4LaIuBT4BS2UsG4qbdaeIpnzMHA4IvZnr/dQD84XJa3KsuaCfWur1FT6cxtOAU5mP4Avv7QWgKmpqQV/7sILL5zd3jnn7wLY/oNfd3GWNiyWzJwR8QJwSFLjN/Bq4Cnct9asVEUfx/AXwN2SlgPTwEepB7b71pqVRBG9qzRrtVpMTEz0bH9F9KL8bOzD5a01I2ky92jNWV4hZJaoomXtwMlnzH7s01nUluLMaZYoB6dZooa2rG1cy7zxxhtnx7aXtK/GtdKdO3cu8U5rxaDfyODMaZaooc2cDYut/On2PvJrdq19+Yw5yJw5zRI19JmzVWtz29N9m4UBLBvAz5l5zpxmiXJwmiVq4MraZicLunma3aVs/w3iZZNmnDnNEjUQmbOTi9H5G6XLuqzS2EeZixAa/wbDklWGgTOnWaIcnGaJWrKszdqT3JsbWgv8DfAVEmkq3U4p1+iq95GV3Z7NfB9ZOf26fQJs3tzdW8Zczg6eIj2EpiJifUSsB94JvArch5tKm5Wq1RNCVwM/joifStoKXJmN3wU8Anyme1MrV+Nm517edO0brK0VrX7mvA64J9t+XVNpIPmm0mZVUjg4s857W4B/bmUHbipt1p5WytprgB9GxIvZ64FoKp0vNZs1lW7cjF24qXR2LTN/8sflrLWjlbL2ek6WtOCm0malKvp8ztOBjcAncsM7GLCm0o0Ml78punEZZLETR80uxzhbWqcKBWdEvAq8cc7Yz6ifvTWzEniFkFmievo4huXLl8e5557r1SxmOX4cg1nF9DQ4L7744raz5tjY2NB0XTMDZ06zZDk4zRJVmU4IPolkw8aZ0yxRDk6zRDk4zRLl4DRLlIPTLFEOTrNEOTjNEuXgNEuUg9MsUYWCU9JfS3pS0gFJ90g6VdIKSfskPZt9PafsyZoNkyWDU9J5wF8CtYhYB4xQb5HpptJmJSpa1i4DTpO0DDgdOAJspd5Mmuzr+7s+O7MhVuRxDP8J7KTexOso8N8R8S0KNpV231qz9hQpa8+hniXPB34HOEPSDUV3EBG7IqIWEbXR0dH2Z2o2ZIrcMvZe4CcRcRxA0l7gPRRsKt1MJw+77bcqz92qpchnzhngckmnSxL1dpgHcVNps1ItmTkjYr+kPcAPgRPA49Qfr3AmbTaVrnLGyc/dj3q3MhVtKn0rcOuc4V/iptJmpfEKIbNEVaaHUIpczlqZnDnNEuXgNEuUg9MsUQ5Os0QlfULIq3FsmDlzmiUq6czpbGnDzJnTLFEOTrNEOTjNEuXgNEuUIqJ3O5OOA78AXurZTsuzkuofh48hDb8bEfPahPQ0OAEkTURErac7LcEgHIePIW0ua80S5eA0S1Q/gnNXH/ZZhkE4Dh9Dwnr+mdPMinFZa5aonganpE2SpiQ9J6kSz1aRtEbSdyUdzB7mtC0br9yDnCSNSHpc0oPZ6yoew9mS9kh6Ovtv8u4qHkcRPQtOSSPA3wPXABcB10u6qFf778AJ4NMR8XbgcuBT2byr+CCnbdR7DjdU8Ri+CDwcEW8DLqF+PFU8jqVFRE/+AO8Gvpl7vR3Y3qv9d/E4vgFsBKaAVdnYKmCq33NbYt6rqf/iXgU8mI1V7RjOAn5Cdq4kN16p4yj6p5dl7XnAodzrw9lYZUgaBy4F9lPwQU4J+QJwE/Cb3FjVjmEtcBz4Ulae3y7pDKp3HIX0MjjVZKwyp4olnQl8HfiriPiffs+nFZKuBY5FxGS/59KhZcA7gNsi4lLqS0EHo4RtopfBeRhYk3u9mvpzPpMn6RTqgXl3ROzNhl/MHuBEqw9y6oMNwBZJzwNfA66S9FWqdQxQ/x06HBH7s9d7qAdr1Y6jkF4G52PABZLOl7Sc+tOx7+/h/tuSPbzpDuBgRHw+963KPMgpIrZHxOqIGKf+7/6diLiBCh0DQES8ABySdGE2dDXwFBU7jqJ6fVfK+6h/9hkB7oyIz/Zs522SdAXwL8CPOPl57Rbqnzt3A2NkD3KKiP/qyyRbIOlK4MaIuFbSG6nYMUhaD9wOLAemgY9STzKVOo4ivELILFFeIWSWKAenWaIcnGaJcnCaJcrBaZYoB6dZohycZolycJol6v8BhhfvO/RC7Z4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(map, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD7CAYAAACR4IPAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPm0lEQVR4nO3df6zddX3H8efL27ICtZaWykpbuZDUCiNr0RuEsS2U2qVekRqCDiLGOX8sS926xQXFP0a2xIUsi9EE4qyAc7MitZaIpAEbbSf7p0LFKFAqHQK9ttKCVBAiS+G9P8733Ht677n3fs853+85n+85r0fSnO/3c+4938+39M37fb4/3l9FBGaWnjf0egJm1pyD0yxRDk6zRDk4zRLl4DRLlIPTLFEdBaekDZIOSDoo6TNFTcrMQO2e55Q0BPwcWA+MAQ8C10XEY8VNz2xwzengdy8GDkbEkwCSvglsBKYNzjMXDcXwirkdbNKs/+z76avPRcSSyeOdBOcy4FDD+hjwzpl+YXjFXH50/4oONmnWf4aWHny62Xgn3znVZGxKjSzpE5IekvTQsedf62BzZoOlk+AcAxrT4HLg8OQfiogtETESESNLFg91sDmzwdJJcD4IrJR0rqRTgGuBe4qZlpm1/Z0zIk5I+iRwPzAE3BERjxY2M7MB18kBISJiJ7CzoLmYWQNfIWSWKAenWaIcnGaJcnCaJcrBaZYoB6dZohycZolycJolqqOLEMxSM7r2mvHlnbu393AmnXPmNEuUM6f1lbzZsgoZ1pnTLFEOTrNEuay1gZRqKdvImdMsUc6c1leqcKAnr1kzp6Q7JB2V9EjD2CJJuyQ9kb2eUe40zQZPnrL2P4ANk8Y+A3w/IlYC38/WzaxAs5a1EfFDScOThjcCl2fLXwP2AJ8ucmJm06mXrs3K1qJL2V6Wye0eEDorIo4AZK9vnu4H3bfWrD25npWSZc57I+LCbP14RCxseP+FiJj1e+fI6nnhju/WLTNl2JQMLT24LyJGJo+3mzmflbQUIHs92snkzGyqdoPzHuDD2fKHge8UMx0zq5v1gJCkO6kd/DlT0hhwE3AzsE3SR4FngPeXOUmrtss//vHx5T1f+UrXtpt6OTubPEdrr5vmrXUFz8XMGvgKIStdWdmyVxm5W3xtrVmiHJxmiXJZa5XVj6VsI2dOs0Q5c1oy+ul2ryI4c5olysFpliiXtQVY+V9/PeP7v7/39SljD9zy5bKmYyXp9oX0zpxmiXLmbNG/PLdqytjwyNj48u9uPXva3z1tx96JlVsKnVbpunGwppPP7UZWq8rN1mZWMgenWaJc1uZUL2d3H3vrlPfesO7QxMrV05e1r1z9zoa1nxQ0s+5I/bxj6vNrhzOnWaKcOWdRP01SPx0yb9PhKT9zcka01FT1yqM8TaVXSNotab+kRyVtzsbdWNqsRHnK2hPApyLifOASYJOkC3BjabNS5WlTcgSo96h9SdJ+YBl93Fi68YqfejlbP0f5+qaJ1p4zndMcBLOVi3nPPfaihWVRc5/pszvdn5YOCGX9ay8C9pKzsbSbSpu1J1dTaQBJ84H/Bj4XETvaaSydelPpyQd/OtF44Oj41uW1seMTn+tra6earSdQVZpEt6qjptKS5gLfBrZGxI5s2I2lzUqUp2+tgNuB/RHx+Ya36o2lb6bCjaWbfb9csG8i6734jpO/Vz7/B0Pjy4sfrZXpzU6vNFr4wdq1t1X8jlpEtsp7KqPVtiOjB0YnPnfVzpa2VQV5znNeBnwI+Jmkn2Rjn8WNpc1Kledo7f8AmuZtN5Y2K4mvEGpicinbqF7KAry0rFbizit9Rr3Tq9IwTzldL2VPGqt4KdvI19aaJWpgM2cRp03e+MtaFn1ux8TpoTOvPjTl56p4IKhIRWWzfsqKeThzmiXKwWmWqIEta4s0/8jEQaKnjy4C4Kxv9fNhovLVS9hm5y2rcqVQp+dcnTnNEuXM2UTjFT+tHsxxxixWs4yTesas6+pdKWbWPQ5Os0QNbFlbbwR9ZOWC8bGlb3pxys/NdFH7oJ+/7LZWDwRV5cDRdJw5zRI1sJmzrlm2zKueVZ1Bu6PVDFh0+5Fuc+Y0S5SD0yxRs/YQkjQP+CHwe9TK4O0RcZOkRcBdwDDwFPCBiHhhps9KvYeQFatKJWQvddJD6FXgiohYDawBNki6BPetNStVnk4IAfw2W52b/Qn6uG+tFWMQMmaZ1UHe7ntDWf+go8CuiMjdt9bM2pMrOCPitYhYAywHLpZ0Yd4NuKm0WXtaOs8ZEccl7QE2kPWtjYgjM/WtjYgtwBaoHRDqcL6WgEE40JN3H8v8O8jzlLElkhZmy6cC7wIeZ6JvLVS4b61ZqvKcSvlDagd8hqgF87aI+GdJi4FtwFvI+tZGxK9n+iyfSjGbarpTKXmO1v6U2sOLJo8/j/vWmpXGVwiZJcrBaT03uvaak/rtWI2D0yxRA3/LmPVeq6cjBuFUDjhzmiXLwWmWKJe1VjlFl7OplsnOnGaJcua0gZdaxqxz5jRLlIPTLFEOTrNEOTjNEuXgNEuUg9MsUQ5Os0TlDs6sA9/Dku7N1hdJ2iXpiez1jPKmaTZ4Wsmcm4H9DetuKm1Worx9a5cD7wFuaxjeSK23ENnr+wqdmdmAy5s5vwDcALzeMJarqbT71pq1J09rzCuBoxGxr50NRMSWiBiJiJEli4fa+QizgZTnwvfLgKskjQLzgAWSvk7OptJm1p5ZM2dE3BgRyyNiGLgW+EFEXI+bSpuVqpPznDcD6yU9AazP1s2sIK0+K2UPtUf9uam0Wcl8hZBZohycZolycJolysFpligHp1miHJxmiXJwmiXKwWmWKAenWaIcnGaJcnCaJcrBaZYoB6dZohycZolycJolysFplqhcN1tLegp4CXgNOBERI5IWAXcBw8BTwAci4oVypmk2eFrJnGsjYk1EjGTrbiptVqJOylo3lTYrUd7gDOB7kvZJ+kQ25qbSZiXK2+Drsog4LOnNwC5Jj+fdQERsAbYAjKyeF23M0Wwg5cqcEXE4ez0K3A1cTNZUGsBNpc2Kl+dxDKdLemN9Gfgz4BHcVNqsVHnK2rOAuyXVf/4bEXGfpAeBbZI+CjwDvL+8aZoNnlmDMyKeBFY3GXdTabMS+Qohs0Q5OM0S5eA0S5SD0yxRDk6zRDk4zRLl4DRLlIPTLFEOTrNEOTjNEuXgNEuUg9MsUQ5Os0Q5OM0S5eA0S1TevrULgduAC6k1+/pL4AAV61s7uvaa8eWdu7f3cCZms8ubOb8I3BcRb6N24/V+3LfWrFR5eggtAP4UuB0gIv4vIo7jvrVmpcpT1p4HHAO+Kmk1sA/YzKS+tVnbzKTlLWXX73/v+PLvbj27pW3M23QYgF3nf7el3zObLE9ZOwd4O/CliLgIeJkWSlg3lTZrT57MOQaMRcTebH07teB8VtLSLGtO27e2Sk2l/+STfwVMZD+A05/+LQD3fXfrtL+34b0fHF+u/++n/lkAD9zy5QJnaYNi1swZEb8CDklalQ2tAx7DfWvNSpX3cQx/A2yVdArwJPARaoHtvrVmJVFE9yrNkdXz4kf3r+ja9vLoRvlZ34bLW2tmaOnBfQ2P1hznK4TMEpW3rO07jRmzF9t0FrXZOHOaJcrBaZaogS1r6+cyh25YUPq26udKX/vXF0vf1iDp9xsZnDnNEjWwmbNupit/it5G4zW71r7GjNnPnDnNEjXwmbNVowdGx5d3rtrZw5kY//5Kr2dQKmdOs0Q5OM0S1XdlbbODBUUeZncp23v9eNqkGWdOs0T1Rebs5GR0443SZZ1WGd9GiRch1P8OBiWrDAJnTrNEOTjNEjVrWZu1J7mrYeg84B+B/ySRptLtlHLjXfXOKXgyTbx8zvzawq3zJwZvKXYbLmf7T54eQgciYk1ErAHeAbwC3I2bSpuVqtUDQuuA/42IpyVtBC7Pxr8G7AE+XdzUylW/2bmbN137BmtrRavfOa8F7syWT2oqDSTfVNqsSnIHZ9Z57yrgW61swE2lzdrTSln7buDHEfFstt4XTaUbS81mTaXrN2PnbiqdnctsfIyDy1lrRytl7XVMlLTgptJmpcrVt1bSacAh4LyI+E02thjYBryFrKl0RPx6ps9JsW9tM36QkXXTdH1rc5W1EfEKsHjS2PPUjt6aWQl8hZBZorr6OIY3nbo0Lh3+C1/NYtbAj2Mwq5iuBufKt77QdtYcXXvNwHRdMwNnTrNkOTjNElWZTgg+iGSDxpnTLFEOTrNEOTjNEuXgNEuUg9MsUQ5Os0Q5OM0S5eA0S5SD0yxRuYJT0t9LelTSI5LulDRP0iJJuyQ9kb2eUfZkzQbJrMEpaRnwt8BIRFwIDFFrkemm0mYlylvWzgFOlTQHOA04DGyk1kya7PV9hc/ObIDleRzDL4F/o9bE6wjwm4j4HjmbSrtvrVl78pS1Z1DLkucCZwOnS7o+7wYiYktEjETEyJLFQ+3P1GzA5Lll7F3ALyLiGICkHcAfkbOpdDOdPOy216o8d6uWPN85nwEukXSaJFFrh7kfN5U2K1XeptL/BPw5cAJ4GPgYMJ8+bSqdlx/1bkXotKn0TcBNk4ZfxU2lzUrjK4TMElWZHkIpcjlrZXLmNEuUg9MsUQ5Os0Q5OM0SlfQBIV+NY4PMmdMsUUlnTmdLG2TOnGaJcnCaJcrBaZYoB6dZonLdMlbYxqRjwMvAc13baHnOpPr74X1IwzkRsWTyYFeDE0DSQ83uXauaftgP70PaXNaaJcrBaZaoXgTnlh5sswz9sB/eh4R1/TunmeXjstYsUV0NTkkbJB2QdFBSJZ6tImmFpN2S9mcPc9qcjVfuQU6ShiQ9LOnebL2K+7BQ0nZJj2f/TS6t4n7k0bXglDQE3Aq8G7gAuE7SBd3afgdOAJ+KiPOBS4BN2byr+CCnzdR6DtdVcR++CNwXEW8DVlPbnyrux+wioit/gEuB+xvWbwRu7Nb2C9yP7wDrgQPA0mxsKXCg13ObZd7Lqf3DvQK4Nxur2j4sAH5BdqykYbxS+5H3TzfL2mXAoYb1sWysMiQNAxcBe8n5IKeEfAG4AXi9Yaxq+3AecAz4alae3ybpdKq3H7l0MzjVZKwyh4olzQe+DfxdRLzY6/m0QtKVwNGI2NfruXRoDvB24EsRcRG1S0H7o4RtopvBOQY0PothObXnfCZP0lxqgbk1InZkw89mD3Ci1Qc59cBlwFWSngK+CVwh6etUax+g9m9oLCL2ZuvbqQVr1fYjl24G54PASknnSjqF2tOx7+ni9tuSPbzpdmB/RHy+4a3KPMgpIm6MiOURMUzt7/0HEXE9FdoHgIj4FXBI0qpsaB3wGBXbj7y6fVfKKLXvPkPAHRHxua5tvE2S/hh4APgZE9/XPkvte2dLD3JKgaTLgX+IiCslLaZi+yBpDXAbcArwJPARakmmUvuRh68QMkuUrxAyS5SD0yxRDk6zRDk4zRLl4DRLlIPTLFEOTrNEOTjNEvX/OOoExzO74rMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(preprocess_observations(map), interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle observations\n",
    "\n",
    "# handle image scaling and resizing to fit the image in input data\n",
    "\n",
    "def preprocess_observations(obs):\n",
    "    # ToDo need love to work\n",
    "    img = obs\n",
    "    img = img.mean(axis=2)\n",
    "    img = (img - 128) / 128 -1\n",
    "\n",
    "    return img.reshape(88,80,1)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create DQN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network\n",
    "input_height = 88\n",
    "input_width = 80\n",
    "input_channels = 1\n",
    "conv_n_maps = [32,64,64]\n",
    "conv_kernel_sizes = [(8,8), (4,4), (3,3)]\n",
    "conv_strides = [4, 2, 1]\n",
    "conv_paddings = ['SAME'] * 3\n",
    "conv_activation = [tf.nn.relu] * 3\n",
    "n_hidden_in = 64 * 11 * 10 # 64 maps with size 11x10\n",
    "n_hidden = 512\n",
    "hidden_activation = tf.nn.relu\n",
    "n_outputs = env.action_space.n# -> ToDo define in enviroment \n",
    "initializer =  tf.keras.initializers.VarianceScaling()# tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "learning_rate = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "def q_network(X_state, name):\n",
    "    prev_layer = X_state\n",
    "    with tf.compat.v1.variable_scope(name) as scope:\n",
    "        for n_maps, kernel_size, strides, padding, activation in zip(\n",
    "            conv_n_maps, conv_kernel_sizes, conv_strides,\n",
    "            conv_paddings, conv_activation):\n",
    "            \n",
    "            prev_layer = tf.compat.v1.layers.conv2d(\n",
    "                prev_layer, filters=n_maps, kernel_size= kernel_size,\n",
    "                strides= strides, padding=padding, activation=activation,\n",
    "                kernel_initializer= initializer)\n",
    "            \n",
    "            last_conv_layer_flat = tf.reshape(prev_layer, shape=[-1, n_hidden_in])\n",
    "            \n",
    "            hidden = tf.compat.v1.layers.dense(last_conv_layer_flat, n_hidden, \n",
    "                                                activation=hidden_activation,\n",
    "                                                kernel_initializer=initializer)\n",
    "            \n",
    "            outputs = tf.compat.v1.layers.dense(hidden, n_outputs, \n",
    "                                                kernel_initializer=initializer)\n",
    "        \n",
    "        trainable_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, \n",
    "                                                    scope=scope.name)\n",
    "        \n",
    "        trainable_vars_by_name = {var.name[len(scope.name):] : var for var in trainable_vars}\n",
    "\n",
    "        return outputs, trainable_vars_by_name\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/keras/legacy_tf_layers/convolutional.py:414: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  warnings.warn('`tf.layers.conv2d` is deprecated and '\n",
      "/opt/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "/opt/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  warnings.warn('`tf.layers.dense` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "X_state = tf.compat.v1.placeholder(tf.float32, shape=[None, input_height, input_width, input_channels])\n",
    "online_q_values, online_vars = q_network(X_state=X_state, name='q_networks/online')\n",
    "target_q_values, target_vars = q_network(X_state=X_state, name='q_networks/target')\n",
    "\n",
    "copy_obs = [target_var.assign(online_vars[var_name]) for var_name, target_var in target_vars.items()]\n",
    "copy_online_to_target = tf.group(*copy_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_action = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
    "q_value = tf.reduce_sum(target_q_values * tf.one_hot(X_action, n_outputs), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])\n",
    "error = tf.abs(y - q_value)\n",
    "clipped_error = tf.clip_by_value(error, 0.0, 1.0)\n",
    "linear_error = 2*(error - clipped_error)\n",
    "loss = tf.compat.v1.reduce_mean(tf.square(clipped_error) +linear_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "optimizer = tf.compat.v1.train.MomentumOptimizer(learning_rate=learning_rate,name='momentum', momentum=0.9, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "saver = tf.compat.v1.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_memory_size = 500000\n",
    "replay_memory = deque([], maxlen=replay_memory_size)\n",
    "\n",
    "def sample_memories(batch_size):\n",
    "    indicies = np.random.permutation(len(replay_memory))[:batch_size]\n",
    "    cols = [[], [], [], []] # state, action, rewards, next_state, continue\n",
    "    for idx in indicies:\n",
    "        memory = replay_memory[idx]\n",
    "        for col, value in zip(cols, memory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return (cols[0], cols[1], cols[2].reshape(-1, 1), cols[3], cols[4].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor explore enviroment\n",
    "\n",
    "eps_min = 0.1\n",
    "eps_max = 1.0\n",
    "eps_decay_steps = 2000000\n",
    "\n",
    "def epsilon_greedy(q_values, step):\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        return np.argmax(q_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 4000000\n",
    "training_start = 10000\n",
    "training_interval = 4\n",
    "save_steps = 1000\n",
    "copy_steps = 10000\n",
    "discont_rate = 0.99\n",
    "skip_start = 90\n",
    "batch_size = 50\n",
    "iteration = 0\n",
    "checkpoint_path = './docking_dqn.ckpt'\n",
    "done = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 01:36:51.667239: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-13 01:36:51.693729: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_path + '.index'):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "        copy_online_to_target.run()\n",
    "    \n",
    "    while True:\n",
    "        step = global_step.eval()\n",
    "        if step >= n_steps:\n",
    "            break\n",
    "        iteration += 1\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            for skip in range(skip_start):\n",
    "                obs, reward, done, info = env.step(0)\n",
    "            state = preprocess_observations(obs)\n",
    "\n",
    "        # online dqn evaluate\n",
    "        q_values = online_q_values.eval(feed_dict={X_state: [state]})\n",
    "        action = epsilon_greedy(q_values, step)\n",
    "\n",
    "        # online dqn play\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        next_state = preprocess_observations(obs)\n",
    "\n",
    "        # remember what happend\n",
    "        replay_memory.append((state, action, reward, next_state, 1.0 - done))\n",
    "        state = next_state\n",
    "\n",
    "        if iteration < training_start or iteration % training_interval != 0:\n",
    "            continue # only train after warmup period and at regular intervals\n",
    "\n",
    "        # get probe from memory\n",
    "        # use target dqn to get target q value\n",
    "        X_state_val, X_action_val, rewards, X_next_state_val, continues = (\n",
    "            sample_memories(batch_size=batch_size))\n",
    "        \n",
    "        next_q_values = target_q_values.eval(\n",
    "            feed_dict={X_state: X_state_val})\n",
    "\n",
    "        max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "        y_val = rewards + continues * discont_rate * max_next_q_values\n",
    "\n",
    "        # train online dqn\n",
    "        training_op.run(feed_dict={X_state: X_state_val, X_action: X_action_val, y: y_val})\n",
    "\n",
    "        # copy online dqn to target dqn\n",
    "        if step % copy_steps == 0:\n",
    "            copy_online_to_target.run()\n",
    "        \n",
    "        # save regulary\n",
    "        if step % save_steps == 0:\n",
    "            saver.save(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cool, actally no big errors....time to work on the enviroment, passing the right values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d2998af58360198af4f829ea7a2e55473ee9c83a4b3343387da68b4c68e0d61"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('DA_Enviroment': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
