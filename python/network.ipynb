{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Docking Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "## !pip install -e ../gym_space_docking/\n",
    "\n",
    "loc = os.popen('pip3 show gym_space_docking').readlines()[7].split()[1]\n",
    "\n",
    "\n",
    "sys.path.append(loc)\n",
    "\n",
    "\n",
    "import gym\n",
    "import os\n",
    "import gym_space_docking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include Network\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create enviroment pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.9.6)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "/home/meyd/Documents/GitHub/Denny-Meyer/IronHack_Final_Project/gym_space_docking/gym_space_docking/envs\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('space_docking-v0')\n",
    "#print(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(7)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "#print(env.observation_space)\n",
    "#print(type(env.action_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "map, reward, done, info = env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(map, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(preprocess_observations(map), interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle observations\n",
    "\n",
    "# handle image scaling and resizing to fit the image in input data\n",
    "\n",
    "def preprocess_observations(obs):\n",
    "    # ToDo need love to work\n",
    "    img = obs\n",
    "    img = img.mean(axis=2)\n",
    "    img = (img - 128) / 128 -1\n",
    "\n",
    "    return img.reshape(160,160,1)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create DQN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network\n",
    "input_height = 160\n",
    "input_width = 160\n",
    "input_channels = 1\n",
    "conv_n_maps = [32,64,64]\n",
    "conv_kernel_sizes = [(8,8), (4,4), (3,3)]\n",
    "conv_strides = [4, 2, 1]\n",
    "conv_paddings = ['SAME'] * 3\n",
    "conv_activation = [tf.nn.relu] * 3\n",
    "n_hidden_in = 64 * 11 * 10 # 64 maps with size 11x10\n",
    "n_hidden = 512\n",
    "hidden_activation = tf.nn.relu\n",
    "n_outputs = env.action_space.n# -> ToDo define in enviroment \n",
    "initializer =  tf.keras.initializers.VarianceScaling()# tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "learning_rate = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "def q_network(X_state, name):\n",
    "    prev_layer = X_state\n",
    "    with tf.compat.v1.variable_scope(name) as scope:\n",
    "        for n_maps, kernel_size, strides, padding, activation in zip(\n",
    "            conv_n_maps, conv_kernel_sizes, conv_strides,\n",
    "            conv_paddings, conv_activation):\n",
    "            \n",
    "            prev_layer = tf.compat.v1.layers.conv2d(\n",
    "                prev_layer, filters=n_maps, kernel_size= kernel_size,\n",
    "                strides= strides, padding=padding, activation=activation,\n",
    "                kernel_initializer= initializer)\n",
    "            \n",
    "            last_conv_layer_flat = tf.reshape(prev_layer, shape=[-1, n_hidden_in])\n",
    "            \n",
    "            hidden = tf.compat.v1.layers.dense(last_conv_layer_flat, n_hidden, \n",
    "                                                activation=hidden_activation,\n",
    "                                                kernel_initializer=initializer)\n",
    "            \n",
    "            outputs = tf.compat.v1.layers.dense(hidden, n_outputs, \n",
    "                                                kernel_initializer=initializer)\n",
    "        \n",
    "        trainable_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, \n",
    "                                                    scope=scope.name)\n",
    "        \n",
    "        trainable_vars_by_name = {var.name[len(scope.name):] : var for var in trainable_vars}\n",
    "\n",
    "        return outputs, trainable_vars_by_name\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/keras/legacy_tf_layers/convolutional.py:414: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  warnings.warn('`tf.layers.conv2d` is deprecated and '\n",
      "/home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "/home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  warnings.warn('`tf.layers.dense` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "X_state = tf.compat.v1.placeholder(tf.float32, shape=[None, input_height, input_width, input_channels])\n",
    "online_q_values, online_vars = q_network(X_state=X_state, name='q_networks/online')\n",
    "target_q_values, target_vars = q_network(X_state=X_state, name='q_networks/target')\n",
    "\n",
    "copy_obs = [target_var.assign(online_vars[var_name]) for var_name, target_var in target_vars.items()]\n",
    "copy_online_to_target = tf.group(*copy_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_action = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
    "q_value = tf.reduce_sum(target_q_values * tf.one_hot(X_action, n_outputs), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])\n",
    "error = tf.abs(y - q_value)\n",
    "clipped_error = tf.clip_by_value(error, 0.0, 1.0)\n",
    "linear_error = 2*(error - clipped_error)\n",
    "loss = tf.compat.v1.reduce_mean(tf.square(clipped_error) +linear_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "optimizer = tf.compat.v1.train.MomentumOptimizer(learning_rate=learning_rate,name='momentum', momentum=0.9, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "saver = tf.compat.v1.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_memory_size = 500000\n",
    "replay_memory = deque([], maxlen=replay_memory_size)\n",
    "\n",
    "def sample_memories(batch_size):\n",
    "    indicies = np.random.permutation(len(replay_memory))[:batch_size]\n",
    "    cols = [[], [], [], []] # state, action, rewards, next_state, continue\n",
    "    for idx in indicies:\n",
    "        memory = replay_memory[idx]\n",
    "        for col, value in zip(cols, memory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return (cols[0], cols[1], cols[2].reshape(-1, 1), cols[3], cols[4].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor explore enviroment\n",
    "\n",
    "eps_min = 0.1\n",
    "eps_max = 1.0\n",
    "eps_decay_steps = 2000000\n",
    "\n",
    "def epsilon_greedy(q_values, step):\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        return np.argmax(q_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 4000000\n",
    "training_start = 10000\n",
    "training_interval = 4\n",
    "save_steps = 1000\n",
    "copy_steps = 10000\n",
    "discont_rate = 0.99\n",
    "skip_start = 90\n",
    "batch_size = 50\n",
    "iteration = 0\n",
    "checkpoint_path = './docking_dqn.ckpt'\n",
    "done = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 22:34:35.102935: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-12-10 22:34:35.103189: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-10 22:34:35.103612: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2021-12-10 22:34:35.112294: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
      "2021-12-10 22:34:35.139725: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2599990000 Hz\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Input to reshape is a tensor with 25600 values, but the requested shape requires a multiple of 7040\n\t [[node q_networks/online/Reshape_2 (defined at tmp/ipykernel_13938/1906098050.py:15) ]]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node q_networks/online/Reshape_2:\n q_networks/online/conv2d_2/Relu (defined at tmp/ipykernel_13938/1906098050.py:10)\n\nOriginal stack trace for 'q_networks/online/Reshape_2':\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n    app.start()\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 677, in start\n    self.io_loop.start()\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n    self._run_once()\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n    handle._run()\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 457, in dispatch_queue\n    await self.process_one()\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 446, in process_one\n    await dispatch(*args)\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 353, in dispatch_shell\n    await result\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 648, in execute_request\n    reply_content = await reply_content\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_cell\n    result = self._run_cell(\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2947, in _run_cell\n    return runner(coro)\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3172, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3364, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"tmp/ipykernel_13938/2287348265.py\", line 2, in <module>\n    online_q_values, online_vars = q_network(X_state=X_state, name='q_networks/online')\n  File \"tmp/ipykernel_13938/1906098050.py\", line 15, in q_network\n    last_conv_layer_flat = tf.reshape(prev_layer, shape=[-1, n_hidden_in])\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\n    return target(*args, **kwargs)\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\", line 195, in reshape\n    result = gen_array_ops.reshape(tensor, shape, name)\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 8377, in reshape\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py\", line 748, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 3528, in _create_op_internal\n    ret = Operation(\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 1990, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1374\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[1;32m   1360\u001b[0m                                       target_list, run_metadata)\n",
      "\u001b[0;32m~/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1450\u001b[0m                           run_metadata):\n\u001b[0;32m-> 1451\u001b[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[1;32m   1452\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 25600 values, but the requested shape requires a multiple of 7040\n\t [[{{node q_networks/online/Reshape_2}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13938/1802260584.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# online dqn evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monline_q_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon_greedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m     \"\"\"\n\u001b[0;32m--> 921\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdeprecation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Use ref() instead.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5510\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5511\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5512\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    968\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1191\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1369\u001b[0m                            run_metadata)\n\u001b[1;32m   1370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1392\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1394\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 25600 values, but the requested shape requires a multiple of 7040\n\t [[node q_networks/online/Reshape_2 (defined at tmp/ipykernel_13938/1906098050.py:15) ]]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node q_networks/online/Reshape_2:\n q_networks/online/conv2d_2/Relu (defined at tmp/ipykernel_13938/1906098050.py:10)\n\nOriginal stack trace for 'q_networks/online/Reshape_2':\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n    app.start()\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 677, in start\n    self.io_loop.start()\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n    self._run_once()\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n    handle._run()\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 457, in dispatch_queue\n    await self.process_one()\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 446, in process_one\n    await dispatch(*args)\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 353, in dispatch_shell\n    await result\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 648, in execute_request\n    reply_content = await reply_content\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_cell\n    result = self._run_cell(\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2947, in _run_cell\n    return runner(coro)\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3172, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3364, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"tmp/ipykernel_13938/2287348265.py\", line 2, in <module>\n    online_q_values, online_vars = q_network(X_state=X_state, name='q_networks/online')\n  File \"tmp/ipykernel_13938/1906098050.py\", line 15, in q_network\n    last_conv_layer_flat = tf.reshape(prev_layer, shape=[-1, n_hidden_in])\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\n    return target(*args, **kwargs)\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\", line 195, in reshape\n    result = gen_array_ops.reshape(tensor, shape, name)\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 8377, in reshape\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py\", line 748, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 3528, in _create_op_internal\n    ret = Operation(\n  File \"home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 1990, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_path + '.index'):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "        copy_online_to_target.run()\n",
    "    \n",
    "    while True:\n",
    "        step = global_step.eval()\n",
    "        if step >= n_steps:\n",
    "            break\n",
    "        iteration += 1\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            for skip in range(skip_start):\n",
    "                obs, reward, done, info = env.step(0)\n",
    "            state = preprocess_observations(obs)\n",
    "\n",
    "        # online dqn evaluate\n",
    "        q_values = online_q_values.eval(feed_dict={X_state: [state]})\n",
    "        action = epsilon_greedy(q_values, step)\n",
    "\n",
    "        # online dqn play\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        next_state = preprocess_observations(obs)\n",
    "\n",
    "        # remember what happend\n",
    "        replay_memory.append((state, action, reward, next_state, 1.0 - done))\n",
    "        state = next_state\n",
    "\n",
    "        if iteration < training_start or iteration % training_interval != 0:\n",
    "            continue # only train after warmup period and at regular intervals\n",
    "\n",
    "        # get probe from memory\n",
    "        # use target dqn to get target q value\n",
    "        X_state_val, X_action_val, rewards, X_next_state_val, continues = (\n",
    "            sample_memories(batch_size=batch_size))\n",
    "        \n",
    "        next_q_values = target_q_values.eval(\n",
    "            feed_dict={X_state: X_state_val})\n",
    "\n",
    "        max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "        y_val = rewards + continues * discont_rate * max_next_q_values\n",
    "\n",
    "        # train online dqn\n",
    "        training_op.run(feed_dict={X_state: X_state_val, X_action: X_action_val, y: y_val})\n",
    "\n",
    "        # copy online dqn to target dqn\n",
    "        if step % copy_steps == 0:\n",
    "            copy_online_to_target.run()\n",
    "        \n",
    "        # save regulary\n",
    "        if step % save_steps == 0:\n",
    "            saver.save(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cool, actally no big errors....time to work on the enviroment, passing the right values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages (21.2.4)\n",
      "Collecting pip\n",
      "  Downloading pip-21.3.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages (58.0.4)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
      "Requirement already satisfied: wheel in /home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages (0.37.0)\n",
      "Installing collected packages: setuptools, pip\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 58.0.4\n",
      "    Uninstalling setuptools-58.0.4:\n",
      "      Successfully uninstalled setuptools-58.0.4\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.2.4\n",
      "    Uninstalling pip-21.2.4:\n",
      "      Successfully uninstalled pip-21.2.4\n",
      "Successfully installed pip-21.3.1 setuptools-59.5.0\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.5.4.60-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.3 MB)\n",
      "     |████████████████████████████████| 60.3 MB 28 kB/s             \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.3 in /home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages (from opencv-python) (1.21.2)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.5.4.60\n",
      "Requirement already satisfied: gym[atari] in /home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages (0.21.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages (from gym[atari]) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages (from gym[atari]) (1.21.2)\n",
      "Requirement already satisfied: ale-py~=0.7.1 in /home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages (from gym[atari]) (0.7.3)\n",
      "Requirement already satisfied: importlib-resources in /home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages (from ale-py~=0.7.1->gym[atari]) (5.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages (from importlib-resources->ale-py~=0.7.1->gym[atari]) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip setuptools wheel\n",
    "!pip install opencv-python\n",
    "!pip install gym[atari]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gym.envs.atari' has no attribute 'AtariEnv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21636/200884337.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Breakout-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Making new env: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'gym.envs.atari' has no attribute 'AtariEnv'"
     ]
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from lib_ploting import ploting\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "env = gym.make('Breakout-v0')\n",
    "\n",
    "# Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions\n",
    "#VALID_ACTIONS = [0, 1, 2, 3, 4,5 ,6 ]\n",
    "# Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions\n",
    "VALID_ACTIONS = [0, 1, 2, 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19359/2255770985.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Action space size: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# print(env.get_action_meanings())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Observation space shape: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Action space size: {}\".format(env.action_space.n))\n",
    "# print(env.get_action_meanings())\n",
    "observation = env.reset()\n",
    "print(\"Observation space shape: {}\".format(observation.shape))\n",
    "plt.figure()\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "[env.step(2) for x in range(1)]\n",
    "plt.figure()\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out what a cropped image looks like\n",
    "plt.imshow(observation[34:-16,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari images. Resizes it and converts it to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
    "            self.output = tf.image.resize_images(\n",
    "                self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "        Returns:\n",
    "            A processed [84, 84, 1] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "        # Placeholders for our input\n",
    "        # Our input are 4 RGB frames of shape 160, 160 each\n",
    "        self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Integer id of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "        X = tf.to_float(self.X_pl) / 255.0\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "        # Three convolutional layers\n",
    "        conv1 = tf.contrib.layers.conv2d(\n",
    "            X, 32, 8, 4, activation_fn=tf.nn.relu)\n",
    "        conv2 = tf.contrib.layers.conv2d(\n",
    "            conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "        conv3 = tf.contrib.layers.conv2d(\n",
    "            conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "        # Fully connected layers\n",
    "        flattened = tf.contrib.layers.flatten(conv3)\n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))\n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "        # Calcualte the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", self.loss),\n",
    "            tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "    def update(self, sess, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_model_parameters(sess, estimator1, estimator2):\n",
    "    \"\"\"\n",
    "    Copies the model parameters of one estimator to another.\n",
    "    Args:\n",
    "      sess: Tensorflow session instance\n",
    "      estimator1: Estimator to copy the paramters from\n",
    "      estimator2: Estimator to copy the parameters to\n",
    "    \"\"\"\n",
    "    e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "    e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "    e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "    e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "    update_ops = []\n",
    "    for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "        op = e2_v.assign(e1_v)\n",
    "        update_ops.append(op)\n",
    "    sess.run(update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=50):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for fff-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Lambda time discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_estimator,\n",
    "        len(VALID_ACTIONS))\n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(sess, state)\n",
    "    state = np.stack([state] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps-1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "        next_state = state_processor.process(sess, next_state)\n",
    "        next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state_processor.process(sess, state)\n",
    "            state = np.stack([state] * 4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "    # Record videos\n",
    "    # Add env Monitor wrapper\n",
    "    env = Monitor(env, directory=monitor_path, video_callable=lambda count: count % record_video_every == 0, resume=True)\n",
    "    for i_episode in range(num_episodes):\n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(sess, state)\n",
    "        state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "            # Add epsilon to Tensorboard\n",
    "            episode_summary = tf.Summary()\n",
    "            episode_summary.value.add(simple_value=epsilon, tag=\"epsilon\")\n",
    "            q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "            # Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                copy_model_parameters(sess, q_estimator, target_estimator)\n",
    "                print(\"\\nCopied model parameters to target network.\")\n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            # Take a step\n",
    "            action_probs = policy(sess, state, epsilon)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "            next_state = state_processor.process(sess, next_state)\n",
    "            next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "            # Save transition to replay memory\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            # Sample a minibatch from the replay memory\n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "            # Calculate q values and targets\n",
    "            # This is where Double Q-Learning comes in!\n",
    "            q_values_next = q_estimator.predict(sess, next_states_batch)\n",
    "            best_actions = np.argmax(q_values_next, axis=1)\n",
    "            q_values_next_target = target_estimator.predict(sess, next_states_batch)\n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * q_values_next_target[np.arange(batch_size), best_actions]\n",
    "\n",
    "            # Perform gradient descent update\n",
    "            states_batch = np.array(states_batch)\n",
    "            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], node_name=\"episode_reward\", tag=\"episode_reward\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], node_name=\"episode_length\", tag=\"episode_length\")\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "        q_estimator.summary_writer.flush()\n",
    "        yield total_t, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14007/594539433.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Where we save our checkpoints and graphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mexperiment_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./experiments/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create a glboal step variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'global_step'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "# Create a glboal step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope=\"q\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "# State processor\n",
    "state_processor = StateProcessor()\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                    env,\n",
    "                                    q_estimator=q_estimator,\n",
    "                                    target_estimator=target_estimator,\n",
    "                                    state_processor=state_processor,\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    num_episodes=10000,\n",
    "                                    replay_memory_size=500000,\n",
    "                                    replay_memory_init_size=50000,\n",
    "                                    update_target_estimator_every=10000,\n",
    "                                    epsilon_start=1.0,\n",
    "                                    epsilon_end=0.1,\n",
    "                                    epsilon_decay_steps=500000,\n",
    "                                    discount_factor=0.99,\n",
    "                                    batch_size=32):\n",
    "        print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d2998af58360198af4f829ea7a2e55473ee9c83a4b3343387da68b4c68e0d61"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('DA_Enviroment': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
