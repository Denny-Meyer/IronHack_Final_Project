{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Docking Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "## !pip install -e ../gym_space_docking/\n",
    "\n",
    "loc = os.popen('pip3 show gym_space_docking').readlines()[7].split()[1]\n",
    "\n",
    "\n",
    "sys.path.append(loc)\n",
    "\n",
    "\n",
    "import gym\n",
    "import os\n",
    "import gym_space_docking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include Network\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create enviroment pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.9.6)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "/home/meyd/Documents/GitHub/Denny-Meyer/IronHack_Final_Project/gym_space_docking/gym_space_docking/envs\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('space_docking-v0')\n",
    "#print(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(7)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "#print(env.observation_space)\n",
    "#print(type(env.action_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle observations\n",
    "\n",
    "# handle image scaling and resizing to fit the image in input data\n",
    "\n",
    "def preprocess_observations(obs):\n",
    "    # ToDo need love to work\n",
    "    img = obs\n",
    "    img = img.mean(axis=2)\n",
    "    img = (img - 128) / 128 -1\n",
    "\n",
    "    return img.reshape(88,80,1)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    map, reward, done, info = env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(map, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(preprocess_observations(map), interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create DQN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network\n",
    "input_height = 88\n",
    "input_width = 80\n",
    "input_channels = 1\n",
    "conv_n_maps = [32,64,64]\n",
    "conv_kernel_sizes = [(8,8), (4,4), (3,3)]\n",
    "conv_strides = [4, 2, 1]\n",
    "conv_paddings = ['SAME'] * 3\n",
    "conv_activation = [tf.nn.relu] * 3\n",
    "n_hidden_in = 64 * 11 * 10 # 64 maps with size 11x10\n",
    "n_hidden = 512\n",
    "hidden_activation = tf.nn.relu\n",
    "n_outputs = env.action_space.n# -> ToDo define in enviroment \n",
    "initializer =  tf.keras.initializers.VarianceScaling()# tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "learning_rate = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "def q_network(X_state, name):\n",
    "    prev_layer = X_state\n",
    "    with tf.compat.v1.variable_scope(name) as scope:\n",
    "        for n_maps, kernel_size, strides, padding, activation in zip(\n",
    "            conv_n_maps, conv_kernel_sizes, conv_strides,\n",
    "            conv_paddings, conv_activation):\n",
    "            \n",
    "            prev_layer = tf.compat.v1.layers.conv2d(\n",
    "                prev_layer, filters=n_maps, kernel_size= kernel_size,\n",
    "                strides= strides, padding=padding, activation=activation,\n",
    "                kernel_initializer= initializer)\n",
    "            \n",
    "            last_conv_layer_flat = tf.reshape(prev_layer, shape=[-1, n_hidden_in])\n",
    "            \n",
    "            hidden = tf.compat.v1.layers.dense(last_conv_layer_flat, n_hidden, \n",
    "                                                activation=hidden_activation,\n",
    "                                                kernel_initializer=initializer)\n",
    "            \n",
    "            outputs = tf.compat.v1.layers.dense(hidden, n_outputs, \n",
    "                                                kernel_initializer=initializer)\n",
    "        \n",
    "        trainable_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, \n",
    "                                                    scope=scope.name)\n",
    "        \n",
    "        trainable_vars_by_name = {var.name[len(scope.name):] : var for var in trainable_vars}\n",
    "\n",
    "        return outputs, trainable_vars_by_name\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/keras/legacy_tf_layers/convolutional.py:414: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  warnings.warn('`tf.layers.conv2d` is deprecated and '\n",
      "/home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "/home/meyd/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  warnings.warn('`tf.layers.dense` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "X_state = tf.compat.v1.placeholder(tf.float32, shape=[None, input_height, input_width, input_channels])\n",
    "online_q_values, online_vars = q_network(X_state=X_state, name='q_networks/online')\n",
    "target_q_values, target_vars = q_network(X_state=X_state, name='q_networks/target')\n",
    "\n",
    "copy_obs = [target_var.assign(online_vars[var_name]) for var_name, target_var in target_vars.items()]\n",
    "copy_online_to_target = tf.group(*copy_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_action = tf.compat.v1.placeholder(tf.int32, shape=[None])\n",
    "q_value = tf.reduce_sum(target_q_values * tf.one_hot(X_action, n_outputs), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])\n",
    "error = tf.abs(y - q_value)\n",
    "clipped_error = tf.clip_by_value(error, 0.0, 1.0)\n",
    "linear_error = 2*(error - clipped_error)\n",
    "loss = tf.compat.v1.reduce_mean(tf.square(clipped_error) +linear_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "optimizer = tf.compat.v1.train.MomentumOptimizer(learning_rate=learning_rate,name='momentum', momentum=0.9, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "saver = tf.compat.v1.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_memory_size = 500000\n",
    "replay_memory = deque([], maxlen=replay_memory_size)\n",
    "\n",
    "def sample_memories(batch_size):\n",
    "    indicies = np.random.permutation(len(replay_memory))[:batch_size]\n",
    "    cols = [[], [], [], [], []] # state, action, rewards, next_state, continue\n",
    "    for idx in indicies:\n",
    "        memory = replay_memory[idx]\n",
    "        for col, value in zip(cols, memory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return (cols[0], cols[1], cols[2].reshape(-1, 1), cols[3], cols[4].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor explore enviroment\n",
    "\n",
    "eps_min = 0.1\n",
    "eps_max = 1.0\n",
    "eps_decay_steps = 2000000\n",
    "\n",
    "def epsilon_greedy(q_values, step):\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        return np.argmax(q_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 4000000\n",
    "training_start = 10000\n",
    "training_interval = 4\n",
    "save_steps = 1000\n",
    "copy_steps = 10000\n",
    "discont_rate = 0.99\n",
    "skip_start = 90\n",
    "batch_size = 50\n",
    "iteration = 0\n",
    "checkpoint_path = './docking_dqn.ckpt'\n",
    "done = True\n",
    "rewards_counter = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./docking_dqn.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 18:00:11.929484: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-12-13 18:00:11.930236: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-13 18:00:11.930712: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2021-12-13 18:00:11.939449: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
      "2021-12-13 18:00:11.964881: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2599990000 Hz\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_path + '.index'):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "        copy_online_to_target.run()\n",
    "    \n",
    "    while True:\n",
    "        step = global_step.eval()\n",
    "        if step >= n_steps:\n",
    "            break\n",
    "        iteration += 1\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            for skip in range(skip_start):\n",
    "                obs, reward, done, info = env.step(0)\n",
    "            state = preprocess_observations(obs)\n",
    "\n",
    "        # online dqn evaluate\n",
    "        q_values = online_q_values.eval(feed_dict={X_state: [state]})\n",
    "        action = epsilon_greedy(q_values, step)\n",
    "\n",
    "        # online dqn play\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        next_state = preprocess_observations(obs)\n",
    "\n",
    "        rewards_counter.append(rewards_counter)\n",
    "\n",
    "        # remember what happend\n",
    "        replay_memory.append((state, action, reward, next_state, 1.0 - done))\n",
    "        state = next_state\n",
    "\n",
    "        if iteration < training_start or iteration % training_interval != 0:\n",
    "            continue # only train after warmup period and at regular intervals\n",
    "\n",
    "        # get probe from memory\n",
    "        # use target dqn to get target q value\n",
    "        X_state_val, X_action_val, rewards, X_next_state_val, continues = (\n",
    "            sample_memories(batch_size=batch_size))\n",
    "        \n",
    "        next_q_values = target_q_values.eval(\n",
    "            feed_dict={X_state: X_state_val})\n",
    "\n",
    "        max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "        y_val = rewards + continues * discont_rate * max_next_q_values\n",
    "\n",
    "        # train online dqn\n",
    "        training_op.run(feed_dict={X_state: X_state_val, X_action: X_action_val, y: y_val})\n",
    "\n",
    "        # copy online dqn to target dqn\n",
    "        if step % copy_steps == 0:\n",
    "            copy_online_to_target.run()\n",
    "        \n",
    "        # save regulary\n",
    "        if step % save_steps == 0:\n",
    "            saver.save(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cool, actally no big errors....time to work on the enviroment, passing the right values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d2998af58360198af4f829ea7a2e55473ee9c83a4b3343387da68b4c68e0d61"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('DA_Enviroment': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
