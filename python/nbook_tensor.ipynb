{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "## !pip install -e ../gym_space_docking/\n",
    "\n",
    "loc = os.popen('pip3 show gym_space_docking').readlines()[7].split()[1]\n",
    "\n",
    "\n",
    "sys.path.append(loc)\n",
    "\n",
    "\n",
    "import gym\n",
    "import os\n",
    "import gym_space_docking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include Network\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]]], dtype=uint8),\n",
       " -0.02,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('space_docking-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "map, reward, done, info = env.step(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map.reshape(88,80,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(map, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n\n",
    "\n",
    "\n",
    "def create_q_model():\n",
    "    # Network defined by the Deepmind paper\n",
    "    inputs = layers.Input(shape=(88, 80, 3,))\n",
    "\n",
    "    # Convolutions on the frames on the screen\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "\n",
    "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('train_model')\n",
    "model_target = keras.models.load_model('target_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = create_q_model()\n",
    "#model_target = create_q_model()\n",
    "\n",
    "#model.compile()\n",
    "#model_target.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration paramaters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon = 1.0  # Epsilon greedy parameter\n",
    "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ")  # Rate at which to reduce chance of random action being taken\n",
    "batch_size = 32  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.279999999999993\n",
      "-3.1799999999999953\n",
      "-6.139999999999939\n",
      "-16.51999999999986\n",
      "-3.2599999999999927\n",
      "-15.699999999999749\n",
      "-18.74000000000001\n",
      "-2.6199999999999966\n",
      "-39.35999999999981\n",
      "-3.6599999999999744\n",
      "38.8800000000006\n",
      "-6.759999999999916\n",
      "-15.75999999999988\n",
      "-7.239999999999907\n",
      "-19.45999999999986\n",
      "-8.759999999999977\n",
      "-24.840000000000376\n",
      "-1.9399999999999968\n",
      "-8.879999999999958\n",
      "-1.4399999999999893\n",
      "-1.399999999999995\n",
      "-1.0799999999999947\n",
      "-4.9999999999999485\n",
      "-1.6000000000000012\n",
      "-2.17999999999999\n",
      "-29.72000000000095\n",
      "-1.0200000000000007\n",
      "running reward: 321.50 at episode 27, frame count 10000\n",
      "-9.419999999999947\n",
      "-6.419999999999977\n",
      "-9.15999999999993\n",
      "-24.119999999999838\n",
      "-4.73999999999998\n",
      "-20.780000000000342\n",
      "55.30000000000112\n",
      "-0.7200000000000004\n",
      "-10.599999999999925\n",
      "-2.4599999999999955\n",
      "-0.9400000000000006\n",
      "-1.1999999999999966\n",
      "-8.379999999999875\n",
      "-8.839999999999952\n",
      "-21.180000000000334\n",
      "-16.97999999999981\n",
      "19.739999999998492\n",
      "-30.119999999999877\n",
      "-11.439999999999806\n",
      "-1.2599999999999882\n",
      "-6.359999999999944\n",
      "running reward: 29.50 at episode 48, frame count 20000\n",
      "33.91999999999788\n",
      "-4.279999999999978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-26 11:20:47.872259: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: train_model/assets\n",
      "INFO:tensorflow:Assets written to: target_model/assets\n",
      "-12.559999999999889\n",
      "-2.1399999999999983\n",
      "-1.9200000000000006\n",
      "-31.260000000000105\n",
      "-2.59999999999998\n",
      "63.600000000000854\n",
      "-3.519999999999979\n",
      "-1.3399999999999963\n",
      "63.44000000000081\n",
      "-48.87999999999961\n",
      "-2.6999999999999913\n",
      "-1.8400000000000005\n",
      "-1.7000000000000004\n",
      "-4.27999999999998\n",
      "-23.87999999999989\n",
      "-23.9400000000024\n",
      "-6.799999999999958\n",
      "-2.2199999999999855\n",
      "-17.639999999999734\n",
      "-8.9399999999999\n",
      "-0.8200000000000005\n",
      "running reward: 1079.30 at episode 71, frame count 30000\n",
      "-31.260000000000467\n",
      "-7.499999999999915\n",
      "-18.04000000000068\n",
      "-1.0000000000000007\n",
      "-1.619999999999989\n",
      "-4.439999999999989\n",
      "-1.6000000000000012\n",
      "-12.679999999999858\n",
      "12.499999999999007\n",
      "14.280000000000236\n",
      "-4.0399999999999805\n",
      "-0.5200000000000002\n",
      "-5.259999999999928\n",
      "48.72000000000116\n",
      "-9.639999999999901\n",
      "-1.8000000000000005\n",
      "-3.0799999999999903\n",
      "-10.559999999999949\n",
      "-1.6199999999999983\n",
      "-12.279999999999834\n",
      "61.34000000000115\n",
      "-8.69999999999994\n",
      "-2.8799999999999883\n",
      "-1.8999999999999995\n",
      "-7.319999999999917\n",
      "-2.7199999999999935\n",
      "-3.4999999999999822\n",
      "-8.479999999999924\n",
      "-1.640000000000001\n",
      "INFO:tensorflow:Assets written to: train_model/assets\n",
      "INFO:tensorflow:Assets written to: target_model/assets\n",
      "-1.2000000000000008\n",
      "-2.199999999999995\n",
      "running reward: 1267.38 at episode 102, frame count 40000\n",
      "-2.079999999999998\n",
      "-1.6199999999999868\n",
      "-2.04\n",
      "-0.6000000000000003\n",
      "-11.099999999999904\n",
      "-2.1599999999999877\n",
      "-12.21999999999985\n",
      "-0.8399999999999994\n",
      "-15.219999999999745\n",
      "-1.8000000000000005\n",
      "-2.9199999999999946\n",
      "-22.540000000000013\n",
      "-6.079999999999953\n",
      "-4.859999999999908\n",
      "-0.7800000000000002\n",
      "-5.57999999999996\n",
      "-1.2399999999999993\n",
      "-11.41999999999985\n",
      "-1.8600000000000005\n",
      "-54.160000000000295\n",
      "-3.1399999999999935\n",
      "-1.9200000000000006\n",
      "-1.0200000000000007\n",
      "-1.6399999999999997\n",
      "-1.0600000000000005\n",
      "-39.5\n",
      "-1.659999999999997\n",
      "-1.1800000000000008\n",
      "-3.0199999999999867\n",
      "-3.2999999999999945\n",
      "-29.58000000000048\n",
      "-23.19999999999984\n",
      "-10.039999999999853\n",
      "-3.8599999999999497\n",
      "running reward: 593.13 at episode 136, frame count 50000\n",
      "-12.63999999999984\n",
      "50.50000000000089\n",
      "-11.0799999999999\n",
      "-5.039999999999937\n",
      "-11.679999999999948\n",
      "-8.259999999999932\n",
      "-5.4199999999999795\n",
      "-8.779999999999845\n",
      "35.68000000000069\n",
      "33.78000000000076\n",
      "-3.1599999999999873\n",
      "-13.679999999999804\n",
      "-7.039999999999896\n",
      "-0.8800000000000006\n",
      "INFO:tensorflow:Assets written to: train_model/assets\n",
      "INFO:tensorflow:Assets written to: target_model/assets\n",
      "-42.80000000000097\n",
      "-3.819999999999971\n",
      "-1.6400000000000003\n",
      "-0.9600000000000006\n",
      "-20.29999999999984\n",
      "-2.2399999999999896\n",
      "-4.339999999999993\n",
      "-3.2599999999999882\n",
      "-5.699999999999985\n",
      "-12.919999999999874\n",
      "-1.0200000000000007\n",
      "-4.659999999999963\n",
      "59.16000000000022\n",
      "-3.5799999999999965\n",
      "-12.299999999999885\n",
      "-3.1599999999999904\n",
      "-4.439999999999984\n",
      "-2.939999999999986\n",
      "-0.7800000000000005\n",
      "running reward: 551.04 at episode 169, frame count 60000\n",
      "-3.13999999999995\n",
      "-4.939999999999938\n",
      "-2.9199999999999977\n",
      "26.059999999998613\n",
      "57.06000000000109\n",
      "-13.139999999999903\n",
      "-4.819999999999972\n",
      "-7.6599999999999415\n",
      "-3.0199999999999925\n",
      "31.779999999998967\n",
      "-19.61999999999985\n",
      "-28.38000000000195\n",
      "-2.6599999999999726\n",
      "-6.159999999999976\n",
      "-8.33999999999996\n",
      "-4.739999999999961\n",
      "-4.959999999999926\n",
      "-3.319999999999986\n",
      "40.40000000000014\n",
      "77.65999999999947\n",
      "-8.419999999999868\n",
      "-6.259999999999966\n",
      "running reward: 1251.34 at episode 191, frame count 70000\n",
      "-35.079999999999934\n",
      "-2.3799999999999875\n",
      "-1.759999999999998\n",
      "-1.759999999999995\n",
      "-20.21999999999992\n",
      "-2.499999999999978\n",
      "-3.3399999999999865\n",
      "-36.200000000000216\n",
      "-2.63999999999998\n",
      "INFO:tensorflow:Assets written to: train_model/assets\n",
      "INFO:tensorflow:Assets written to: target_model/assets\n",
      "-4.279999999999973\n",
      "-59.71999999999948\n",
      "33.06000000000029\n",
      "-41.02000000000044\n",
      "78.05999999999959\n",
      "-6.099999999999962\n",
      "-17.259999999999955\n",
      "-5.979999999999961\n",
      "30.319999999999308\n",
      "-1.1999999999999997\n",
      "-12.819999999999784\n",
      "running reward: 998.69 at episode 211, frame count 80000\n",
      "-29.260000000001188\n",
      "-1.1200000000000008\n",
      "-15.299999999999944\n",
      "-0.7400000000000004\n",
      "-6.319999999999946\n",
      "-9.099999999999914\n",
      "-1.8200000000000005\n",
      "-32.900000000000006\n",
      "-9.97999999999987\n",
      "-7.7399999999999824\n",
      "-4.5199999999999845\n",
      "63.64000000000061\n",
      "-1.819999999999999\n",
      "-5.979999999999913\n",
      "-5.139999999999981\n",
      "-23.72000000000002\n",
      "-1.12\n",
      "-10.379999999999876\n",
      "-6.199999999999955\n",
      "-11.919999999999888\n",
      "-1.92\n",
      "18.319999999998135\n",
      "running reward: 1716.41 at episode 233, frame count 90000\n",
      "-11.799999999999837\n",
      "-32.00000000000074\n",
      "-7.2399999999999\n",
      "-1.279999999999999\n",
      "-2.0599999999999983\n",
      "-6.259999999999954\n",
      "-9.299999999999967\n",
      "-1.899999999999999\n",
      "-29.12000000000009\n",
      "-16.579999999999988\n",
      "-5.239999999999967\n",
      "-0.8600000000000005\n",
      "-7.779999999999905\n",
      "-12.779999999999921\n",
      "64.99999999999999\n",
      "-2.7999999999999874\n",
      "-1.3599999999999932\n",
      "INFO:tensorflow:Assets written to: train_model/assets\n",
      "INFO:tensorflow:Assets written to: target_model/assets\n",
      "42.54000000000025\n",
      "15.659999999999796\n",
      "-0.9799999999999995\n",
      "-39.34000000000111\n",
      "-3.7399999999999842\n",
      "-0.7000000000000004\n",
      "24.919999999998886\n",
      "-1.0200000000000007\n",
      "running reward: 1433.00 at episode 258, frame count 100000\n",
      "-18.320000000000007\n",
      "-28.66000000000005\n",
      "40.21999999999985\n",
      "-34.57999999999994\n",
      "59.68000000000113\n",
      "-14.459999999999916\n",
      "55.080000000000176\n",
      "-0.9400000000000006\n",
      "-6.6399999999999775\n",
      "-2.619999999999985\n",
      "-9.539999999999893\n",
      "-1.7999999999999905\n",
      "-3.9399999999999844\n",
      "53.16000000000053\n",
      "-7.399999999999934\n",
      "54.46000000000063\n",
      "-26.880000000000145\n",
      "68.31999999999992\n",
      "running reward: 1849.07 at episode 276, frame count 110000\n",
      "-9.759999999999854\n",
      "-1.7200000000000004\n",
      "-12.939999999999849\n",
      "-3.9399999999999693\n",
      "-3.859999999999985\n",
      "-5.039999999999978\n",
      "-12.35999999999979\n",
      "-3.059999999999987\n",
      "-27.380000000000813\n",
      "-0.8400000000000005\n",
      "-14.679999999999906\n",
      "-5.379999999999959\n",
      "-1.0599999999999983\n",
      "-53.93999999999878\n",
      "-3.3399999999999777\n",
      "-1.140000000000001\n",
      "-4.819999999999982\n",
      "-1.08\n",
      "-1.5\n",
      "64.90000000000018\n",
      "-1.240000000000001\n",
      "-27.56000000000125\n",
      "-4.139999999999984\n",
      "-58.60000000000025\n",
      "INFO:tensorflow:Assets written to: train_model/assets\n",
      "INFO:tensorflow:Assets written to: target_model/assets\n",
      "running reward: 999.25 at episode 300, frame count 120000\n",
      "-7.5999999999999615\n",
      "-1.3800000000000008\n",
      "-4.579999999999931\n",
      "-2.0199999999999925\n",
      "-3.379999999999993\n",
      "-1.6799999999999997\n",
      "-12.43999999999991\n",
      "-2.359999999999993\n",
      "-2.639999999999997\n",
      "-8.459999999999898\n",
      "-17.70000000000015\n",
      "-22.28000000000064\n",
      "-10.119999999999854\n",
      "-2.459999999999974\n",
      "-3.179999999999995\n",
      "-1.5000000000000004\n",
      "-23.860000000000138\n",
      "-11.619999999999951\n",
      "-4.099999999999993\n",
      "-4.5399999999999725\n",
      "-33.11999999999981\n",
      "-14.71999999999995\n",
      "5.460000000000463\n",
      "-28.160000000000135\n",
      "-4.559999999999986\n",
      "-3.55999999999999\n",
      "-1.159999999999997\n",
      "-0.9400000000000006\n",
      "-4.059999999999955\n",
      "-2.6199999999999957\n",
      "-4.379999999999979\n",
      "-9.399999999999912\n",
      "-1.279999999999998\n",
      "-1.5199999999999996\n",
      "-10.459999999999933\n",
      "running reward: 405.14 at episode 335, frame count 130000\n",
      "-23.059999999999878\n",
      "-2.9399999999999955\n",
      "-1.0599999999999983\n",
      "-0.6600000000000004\n",
      "-9.299999999999905\n",
      "-22.22000000000076\n",
      "-9.239999999999908\n",
      "-3.579999999999985\n",
      "22.319999999998736\n",
      "-26.13999999999983\n",
      "-1.8200000000000003\n",
      "-6.999999999999906\n",
      "-6.379999999999942\n",
      "-4.1599999999999735\n",
      "60.320000000000746\n",
      "INFO:tensorflow:Assets written to: train_model/assets\n",
      "INFO:tensorflow:Assets written to: target_model/assets\n",
      "-72.82000000000252\n",
      "-2.9399999999999897\n",
      "55.72000000000018\n",
      "running reward: 75.21 at episode 353, frame count 140000\n",
      "-28.559999999999906\n",
      "68.53999999999857\n",
      "-11.699999999999854\n",
      "-2.7599999999999953\n",
      "-4.8799999999999155\n",
      "-1.8000000000000007\n",
      "-21.180000000000334\n",
      "-1.2199999999999975\n",
      "-10.479999999999915\n",
      "-21.800000000000143\n",
      "-3.439999999999987\n",
      "-3.0999999999999943\n",
      "-3.1599999999999975\n",
      "-68.1199999999968\n",
      "-1.92\n",
      "-0.9600000000000004\n",
      "-7.55999999999994\n",
      "46.060000000000485\n",
      "86.65999999999951\n",
      "running reward: -718.59 at episode 372, frame count 150000\n",
      "-57.75999999999851\n",
      "-13.619999999999813\n",
      "-2.5599999999999965\n",
      "29.2599999999998\n",
      "-4.839999999999968\n",
      "-10.39999999999998\n",
      "-2.0399999999999996\n",
      "-14.439999999999841\n",
      "-27.079999999999867\n",
      "-16.299999999999855\n",
      "-30.340000000000195\n",
      "-16.91999999999984\n",
      "-2.4799999999999995\n",
      "-13.879999999999898\n",
      "52.42000000000097\n",
      "-3.1599999999999886\n",
      "-0.8000000000000005\n",
      "-1.1200000000000008\n",
      "-4.419999999999985\n",
      "-1.3599999999999968\n",
      "-3.5799999999999343\n",
      "running reward: -903.64 at episode 393, frame count 160000\n",
      "-14.179999999999872\n",
      "-5.5599999999999685\n",
      "-1.8599999999999839\n",
      "-9.539999999999976\n",
      "72.0399999999999\n",
      "-5.359999999999956\n",
      "-3.099999999999982\n",
      "INFO:tensorflow:Assets written to: train_model/assets\n",
      "INFO:tensorflow:Assets written to: target_model/assets\n",
      "-3.979999999999942\n",
      "-2.099999999999999\n",
      "-18.320000000000686\n",
      "-21.55999999999993\n",
      "47.50000000000068\n",
      "-3.659999999999987\n",
      "46.54000000000046\n",
      "-16.65999999999989\n",
      "-37.49999999999978\n",
      "-15.819999999999926\n",
      "-2.3399999999999874\n",
      "-1.9000000000000017\n",
      "-4.13999999999999\n",
      "-11.799999999999944\n",
      "running reward: -688.43 at episode 414, frame count 170000\n",
      "-41.73999999999963\n",
      "-7.559999999999976\n",
      "-3.17999999999999\n",
      "-6.579999999999973\n",
      "-0.6000000000000003\n",
      "-2.3199999999999945\n",
      "-2.3399999999999856\n",
      "-0.8799999999999981\n",
      "-7.739999999999933\n",
      "-0.9000000000000006\n",
      "40.92000000000035\n",
      "-4.879999999999966\n",
      "-23.679999999999833\n",
      "-21.380000000000173\n",
      "-1.6200000000000003\n",
      "-1.160000000000001\n",
      "-1.5400000000000003\n",
      "-0.7000000000000004\n",
      "-1.1600000000000001\n",
      "-5.319999999999976\n",
      "-3.2799999999999727\n",
      "-31.70000000000072\n",
      "-2.779999999999969\n",
      "53.06000000000058\n",
      "-1.1000000000000008\n",
      "-7.219999999999883\n",
      "-18.34000000000076\n",
      "-30.02000000000055\n",
      "running reward: -310.06 at episode 442, frame count 180000\n",
      "-7.039999999999944\n",
      "-3.299999999999992\n",
      "-6.979999999999951\n",
      "-15.619999999999928\n",
      "-3.0399999999999716\n",
      "-9.839999999999963\n",
      "-5.199999999999986\n",
      "-2.4599999999999884\n",
      "INFO:tensorflow:Assets written to: train_model/assets\n",
      "INFO:tensorflow:Assets written to: target_model/assets\n",
      "-3.3999999999999924\n",
      "-22.45999999999989\n",
      "-3.6199999999999743\n",
      "-18.159999999999865\n",
      "-37.85999999999996\n",
      "-13.119999999999894\n",
      "-19.700000000000035\n",
      "-1.8600000000000005\n",
      "28.379999999999153\n",
      "-2.8400000000000003\n",
      "-3.959999999999992\n",
      "-6.19999999999998\n",
      "-3.8799999999999955\n",
      "-0.9999999999999942\n",
      "-29.36000000000036\n",
      "-2.8599999999999897\n",
      "-14.839999999999867\n",
      "-1.6000000000000003\n",
      "-1.2799999999999985\n",
      "-10.979999999999858\n",
      "-3.0599999999999907\n",
      "-1.3599999999999879\n",
      "-6.359999999999936\n",
      "-2.959999999999993\n",
      "-38.16000000000237\n",
      "running reward: -223.21 at episode 475, frame count 190000\n",
      "-14.379999999999795\n",
      "-3.47999999999998\n",
      "4.480000000000197\n",
      "-4.759999999999991\n",
      "-2.679999999999999\n",
      "-3.0599999999999854\n",
      "-2.000000000000001\n",
      "-9.000000000000124\n",
      "-4.419999999999984\n",
      "46.06000000000058\n",
      "-11.899999999999944\n",
      "-2.6199999999999957\n",
      "-11.919999999999867\n",
      "-13.659999999999874\n",
      "-25.88000000000035\n",
      "-3.619999999999993\n",
      "-16.079999999999725\n",
      "-6.759999999999939\n",
      "-1.9000000000000006\n",
      "-4.399999999999991\n",
      "-2.7\n",
      "-13.91999999999992\n",
      "-6.619999999999937\n",
      "74.51999999999947\n",
      "-4.039999999999982\n",
      "INFO:tensorflow:Assets written to: train_model/assets\n",
      "INFO:tensorflow:Assets written to: target_model/assets\n",
      "-58.03999999999929\n",
      "-22.58000000000062\n",
      "running reward: -243.93 at episode 502, frame count 200000\n",
      "65.14000000000046\n",
      "-10.619999999999905\n",
      "-32.24000000000236\n",
      "-28.080000000000073\n",
      "-1.9200000000000006\n",
      "-4.6399999999999935\n",
      "-1.6200000000000003\n",
      "-0.4400000000000002\n",
      "-2.7799999999999887\n",
      "-1.2000000000000008\n",
      "-11.959999999999921\n",
      "-6.219999999999976\n",
      "-7.539999999999974\n",
      "-28.420000000000794\n",
      "-8.279999999999948\n",
      "-2.3999999999999866\n",
      "-2.119999999999998\n",
      "-1.3999999999999941\n",
      "-2.83999999999999\n",
      "-28.200000000000617\n",
      "-9.619999999999937\n",
      "-19.500000000000178\n",
      "-10.179999999999978\n",
      "-2.079999999999992\n",
      "-3.6399999999999766\n",
      "-0.6400000000000003\n",
      "-15.319999999999894\n",
      "-6.359999999999986\n",
      "52.379999999999804\n",
      "running reward: -93.17 at episode 531, frame count 210000\n",
      "-78.78000000000107\n",
      "-0.6800000000000004\n",
      "-4.419999999999962\n",
      "31.18000000000027\n",
      "-26.22000000000045\n",
      "-11.539999999999951\n",
      "-1.0800000000000007\n",
      "-2.55999999999999\n",
      "-2.5399999999999996\n",
      "-3.239999999999978\n",
      "-6.319999999999954\n",
      "-6.999999999999905\n",
      "59.24000000000067\n",
      "-4.159999999999957\n",
      "-4.479999999999981\n",
      "-17.380000000000614\n",
      "76.51999999999927\n",
      "-4.199999999999991\n",
      "-1.779999999999999\n",
      "INFO:tensorflow:Assets written to: train_model/assets\n",
      "INFO:tensorflow:Assets written to: target_model/assets\n",
      "-2.4199999999999866\n",
      "-0.9000000000000006\n",
      "-1.5800000000000003\n",
      "43.60000000000038\n",
      "-4.639999999999986\n",
      "-2.6599999999999655\n",
      "-4.079999999999989\n",
      "-6.0199999999999765\n",
      "-1.3600000000000008\n",
      "-2.679999999999998\n",
      "-2.859999999999994\n",
      "51.260000000000424\n",
      "-33.46000000000018\n",
      "41.52000000000052\n",
      "-5.699999999999976\n",
      "running reward: 501.16 at episode 565, frame count 220000\n",
      "-5.479999999999958\n",
      "-24.700000000001264\n",
      "-37.860000000000355\n",
      "-8.9599999999999\n",
      "-12.79999999999984\n",
      "-6.159999999999988\n",
      "-15.979999999999865\n",
      "-2.579999999999991\n",
      "-5.179999999999964\n",
      "-6.019999999999945\n",
      "-1.6800000000000002\n",
      "43.08000000000045\n",
      "-9.539999999999955\n",
      "-13.859999999999884\n",
      "-2.9999999999999964\n",
      "-27.90000000000051\n",
      "-2.559999999999987\n",
      "-3.419999999999991\n",
      "-3.8599999999999923\n",
      "-15.599999999999902\n",
      "-1.8200000000000007\n",
      "-23.30000000000005\n",
      "-19.900000000000084\n",
      "-1.5400000000000003\n",
      "30.240000000000286\n",
      "-1.359999999999999\n",
      "running reward: 475.15 at episode 591, frame count 230000\n",
      "71.23999999999971\n",
      "-1.5000000000000009\n",
      "-0.7199999999999998\n",
      "-0.7199999999999993\n",
      "-26.920000000000854\n",
      "-1.359999999999998\n",
      "-3.159999999999995\n",
      "-27.380000000000464\n",
      "-9.05999999999996\n",
      "INFO:tensorflow:Assets written to: train_model/assets\n",
      "INFO:tensorflow:Assets written to: target_model/assets\n",
      "-89.52000000000123\n",
      "-2.9799999999999973\n",
      "-26.34000000000009\n",
      "-65.57999999999983\n",
      "-11.999999999999968\n",
      "-0.6000000000000003\n",
      "-27.019999999999893\n",
      "-6.559999999999971\n",
      "15.859999999999575\n",
      "-3.8599999999999897\n",
      "running reward: -35.64 at episode 610, frame count 240000\n",
      "31.499999999999936\n",
      "-1.6200000000000003\n",
      "53.02000000000037\n",
      "-3.4799999999999933\n",
      "-0.9000000000000006\n",
      "-19.879999999999985\n",
      "-1.2599999999999958\n",
      "-0.8000000000000007\n",
      "-2.6399999999999992\n",
      "60.84000000000056\n",
      "-4.0199999999999845\n",
      "-9.21999999999998\n",
      "-1.9200000000000006\n",
      "-0.7000000000000004\n",
      "-6.499999999999949\n",
      "-4.159999999999983\n",
      "-3.639999999999974\n",
      "34.27999999999907\n",
      "72.75999999999937\n",
      "-1.6400000000000012\n",
      "-2.85999999999998\n",
      "-3.0999999999999908\n",
      "-4.699999999999986\n",
      "-37.599999999999774\n",
      "-6.579999999999986\n",
      "-76.18000000000364\n",
      "-4.979999999999941\n",
      "-1.7200000000000004\n",
      "-2.8799999999999915\n",
      "-0.6000000000000003\n",
      "-14.699999999999918\n",
      "-1.7200000000000004\n",
      "-2.979999999999995\n",
      "-2.3799999999999644\n",
      "running reward: 555.18 at episode 644, frame count 250000\n",
      "-36.05999999999963\n",
      "-8.559999999999974\n",
      "-2.4799999999999978\n",
      "-4.359999999999983\n",
      "55.26000000000057\n",
      "-22.4000000000001\n",
      "INFO:tensorflow:Assets written to: train_model/assets\n",
      "INFO:tensorflow:Assets written to: target_model/assets\n",
      "-15.95999999999977\n",
      "-5.219999999999971\n",
      "-9.09999999999997\n",
      "-2.0799999999999987\n",
      "-6.119999999999925\n",
      "-4.379999999999989\n",
      "-35.520000000000074\n",
      "-2.3599999999999928\n",
      "21.839999999998543\n",
      "-13.379999999999976\n",
      "-5.65999999999998\n",
      "-31.160000000000682\n",
      "-1.5000000000000004\n",
      "75.0999999999992\n",
      "-3.3999999999999844\n",
      "-0.7800000000000007\n",
      "-1.6800000000000004\n",
      "-2.959999999999993\n",
      "-5.019999999999957\n",
      "-6.8999999999999115\n",
      "-22.059999999999874\n",
      "-3.8599999999999914\n",
      "-14.880000000000262\n",
      "-0.5200000000000002\n",
      "-2.2399999999999753\n",
      "-13.55999999999984\n",
      "running reward: 429.93 at episode 676, frame count 260000\n",
      "-0.5600000000000003\n",
      "-8.87999999999993\n",
      "-27.460000000000434\n",
      "-3.3599999999999897\n",
      "-4.579999999999979\n",
      "20.8399999999983\n",
      "-7.159999999999961\n",
      "-23.960000000000058\n",
      "-2.0999999999999983\n",
      "-24.860000000000174\n",
      "33.64000000000014\n",
      "59.10000000000098\n",
      "-3.339999999999992\n",
      "-2.6999999999999957\n",
      "71.55999999999983\n",
      "-34.11999999999996\n",
      "-5.340000000000269\n",
      "-2.01999999999998\n",
      "-2.7399999999999913\n",
      "-2.739999999999993\n",
      "running reward: 254.75 at episode 696, frame count 270000\n",
      "-27.00000000000044\n",
      "-8.799999999999967\n",
      "-2.579999999999998\n",
      "70.91999999999962\n",
      "INFO:tensorflow:Assets written to: train_model/assets\n",
      "INFO:tensorflow:Assets written to: target_model/assets\n",
      "-6.459999999999985\n",
      "-4.159999999999982\n",
      "-1.2000000000000008\n",
      "-4.659999999999993\n",
      "-1.8399999999999888\n",
      "83.43999999999942\n",
      "-12.040000000000335\n",
      "-13.980000000000585\n",
      "-2.0399999999999983\n",
      "66.57999999999959\n",
      "-24.999999999999872\n",
      "-5.019999999999993\n",
      "-4.299999999999997\n",
      "9.64000000000043\n",
      "-6.739999999999932\n",
      "-5.379999999999952\n",
      "-2.9599999999999986\n",
      "39.44000000000061\n",
      "-26.140000000001482\n",
      "running reward: 1441.40 at episode 719, frame count 280000\n",
      "-36.279999999999895\n",
      "-5.539999999999769\n",
      "-8.939999999999932\n",
      "-4.639999999999993\n",
      "-1.1200000000000008\n",
      "44.08000000000101\n",
      "-14.999999999999888\n",
      "-1.9400000000000006\n",
      "-5.499999999999963\n",
      "-0.6800000000000006\n",
      "-6.259999999999979\n",
      "-32.60000000000172\n",
      "-6.779999999999877\n",
      "-4.11999999999999\n",
      "-9.579999999999975\n",
      "-13.639999999999802\n",
      "-4.5599999999999365\n",
      "-13.199999999999855\n",
      "-10.439999999999952\n",
      "-34.26000000000015\n",
      "-36.42000000000016\n",
      "-1.6600000000000001\n",
      "-1.8599999999999954\n",
      "-3.87999999999999\n",
      "-2.799999999999992\n",
      "-8.099999999999966\n",
      "-2.6799999999999953\n",
      "-4.459999999999979\n",
      "-8.099999999999923\n",
      "running reward: 1354.06 at episode 748, frame count 290000\n",
      "37.00000000000011\n",
      "-1.0800000000000007\n",
      "INFO:tensorflow:Assets written to: train_model/assets\n",
      "INFO:tensorflow:Assets written to: target_model/assets\n",
      "39.16000000000056\n",
      "-3.8999999999999835\n",
      "-1.479999999999992\n",
      "29.920000000000478\n",
      "-7.239999999999956\n",
      "-10.259999999999923\n",
      "-14.799999999999898\n",
      "-3.839999999999976\n",
      "-8.139999999999958\n",
      "-12.259999999999827\n",
      "50.14000000000046\n",
      "-3.6999999999999718\n",
      "-18.199999999999825\n",
      "-45.32000000000125\n",
      "-11.599999999999916\n",
      "-1.7200000000000004\n",
      "-2.3599999999999763\n",
      "-17.439999999999838\n",
      "-4.239999999999961\n",
      "-4.359999999999972\n",
      "-52.79999999999978\n",
      "running reward: 1733.17 at episode 771, frame count 300000\n",
      "52.28000000000137\n",
      "77.15999999999948\n",
      "-4.919999999999976\n",
      "-21.519999999999975\n",
      "-2.679999999999991\n",
      "-5.139999999999987\n",
      "-3.899999999999994\n",
      "-6.499999999999941\n",
      "49.260000000000424\n",
      "-6.319999999999911\n",
      "-2.3199999999999736\n",
      "28.12000000000011\n",
      "-29.380000000000805\n",
      "-49.21999999999992\n",
      "-4.159999999999994\n",
      "-1.6199999999999992\n",
      "-27.959999999999972\n",
      "-0.8400000000000005\n",
      "-2.9199999999999977\n",
      "-16.15999999999993\n",
      "-8.17999999999998\n",
      "-11.27999999999986\n",
      "-37.57999999999998\n",
      "-7.4399999999998965\n",
      "-4.479999999999962\n",
      "running reward: 2069.70 at episode 796, frame count 310000\n",
      "46.639999999999844\n",
      "-8.299999999999926\n",
      "-2.559999999999995\n",
      "74.45999999999972\n",
      "INFO:tensorflow:Assets written to: train_model/assets\n",
      "INFO:tensorflow:Assets written to: target_model/assets\n",
      "-13.819999999999828\n",
      "-12.219999999999864\n",
      "-6.179999999999984\n",
      "-4.959999999999991\n",
      "-9.21999999999998\n",
      "-58.099999999998666\n",
      "61.300000000001106\n",
      "-3.8199999999999967\n",
      "-2.9999999999999942\n",
      "-2.3599999999999928\n",
      "-9.139999999999935\n",
      "-36.220000000000105\n",
      "-6.959999999999969\n",
      "-3.6999999999999895\n",
      "-0.6600000000000004\n",
      "-0.5600000000000003\n",
      "-63.9999999999996\n",
      "-26.640000000000235\n",
      "-2.9399999999999817\n",
      "-0.9800000000000006\n",
      "-44.76000000000001\n",
      "-11.279999999999852\n",
      "-1.360000000000001\n",
      "-11.299999999999946\n",
      "-2.9599999999999946\n",
      "63.179999999999794\n",
      "-2.3799999999999972\n",
      "-0.8600000000000005\n",
      "-2.399999999999992\n",
      "-4.019999999999961\n",
      "running reward: 886.46 at episode 830, frame count 320000\n",
      "-32.060000000001565\n",
      "-131.2200000000134\n",
      "-5.739999999999974\n",
      "-4.3599999999999905\n",
      "-3.7399999999999736\n",
      "-40.0199999999999\n",
      "-6.2199999999999065\n",
      "-10.739999999999943\n",
      "-17.220000000000837\n",
      "-3.0999999999999748\n",
      "-5.639999999999979\n",
      "-2.5999999999999877\n",
      "-1.0999999999999925\n",
      "-7.039999999999942\n",
      "-8.859999999999925\n",
      "-2.579999999999997\n",
      "-41.23999999999984\n",
      "-17.440000000000023\n",
      "-51.60000000000066\n",
      "running reward: -1783.97 at episode 849, frame count 330000\n",
      "-47.26000000000131\n",
      "INFO:tensorflow:Assets written to: train_model/assets\n",
      "INFO:tensorflow:Assets written to: target_model/assets\n",
      "-0.5400000000000003\n",
      "-7.079999999999928\n",
      "-1.6200000000000003\n",
      "74.87999999999876\n",
      "-1.1599999999999997\n",
      "-2.919999999999998\n",
      "-1.7799999999999905\n",
      "-1.4600000000000002\n",
      "-25.06000000000023\n",
      "-3.379999999999984\n",
      "-2.779999999999991\n",
      "-2.8399999999999874\n",
      "-1.9800000000000006\n",
      "-8.499999999999975\n",
      "-9.599999999999968\n",
      "-5.659999999999979\n",
      "-1.4999999999999916\n",
      "-0.6000000000000003\n",
      "-40.35999999999984\n",
      "-7.099999999999933\n",
      "-2.779999999999994\n",
      "-4.279999999999974\n",
      "-6.339999999999981\n",
      "-2.859999999999993\n",
      "-2.0999999999999983\n",
      "-42.120000000001234\n",
      "-7.179999999999911\n",
      "-0.6400000000000003\n",
      "-1.939999999999999\n",
      "-1.799999999999998\n",
      "-7.419999999999975\n",
      "-4.099999999999959\n",
      "-17.860000000000724\n",
      "48.940000000000865\n",
      "-0.9800000000000009\n",
      "-14.13999999999992\n",
      "-3.5999999999999934\n",
      "-5.259999999999978\n",
      "-51.919999999998225\n",
      "running reward: -2679.79 at episode 889, frame count 340000\n",
      "-21.300000000000104\n",
      "-3.27999999999998\n",
      "90.09999999999944\n",
      "-1.5799999999999992\n",
      "-2.5999999999999956\n",
      "47.86000000000048\n",
      "-4.739999999999991\n",
      "-4.739999999999975\n",
      "-0.5400000000000004\n",
      "-23.620000000001024\n",
      "-3.1399999999999952\n",
      "INFO:tensorflow:Assets written to: train_model/assets\n",
      "INFO:tensorflow:Assets written to: target_model/assets\n",
      "-5.959999999999985\n",
      "-0.5200000000000002\n",
      "-5.419999999999955\n",
      "68.10000000000043\n",
      "-4.219999999999987\n",
      "-5.5199999999999925\n",
      "-8.69999999999997\n",
      "-27.98000000000001\n",
      "-15.859999999999905\n",
      "-14.10000000000017\n",
      "-9.519999999999923\n",
      "-50.939999999999564\n",
      "-6.059999999999981\n",
      "-2.039999999999991\n",
      "-41.799999999999905\n",
      "-23.78000000000053\n",
      "40.679999999999666\n",
      "running reward: -2197.17 at episode 917, frame count 350000\n",
      "9.600000000000563\n",
      "-1.5400000000000003\n",
      "-12.319999999999965\n",
      "57.82000000000036\n",
      "-6.6199999999999655\n",
      "-0.8200000000000005\n",
      "-6.719999999999983\n",
      "-3.17999999999999\n",
      "-2.999999999999969\n",
      "-6.719999999999951\n",
      "-6.599999999999924\n",
      "-5.419999999999928\n",
      "-2.259999999999983\n",
      "-3.919999999999993\n",
      "-9.019999999999973\n",
      "-11.699999999999946\n",
      "-33.78000000000022\n",
      "-23.61999999999993\n",
      "-1.360000000000001\n",
      "-2.3999999999999915\n",
      "-0.8600000000000005\n",
      "-1.1600000000000008\n",
      "-0.6000000000000003\n",
      "-2.519999999999995\n",
      "-2.6399999999999926\n",
      "-1.5599999999999978\n",
      "-21.219999999999928\n",
      "-5.619999999999958\n",
      "-1.1800000000000008\n",
      "-0.46000000000000024\n",
      "-2.1399999999999983\n",
      "-4.179999999999943\n",
      "-5.979999999999977\n",
      "INFO:tensorflow:Assets written to: train_model/assets\n",
      "INFO:tensorflow:Assets written to: target_model/assets\n",
      "-0.7200000000000004\n",
      "running reward: 612.74 at episode 951, frame count 360000\n",
      "3.660000000000208\n",
      "-3.3399999999999954\n",
      "56.84000000000049\n",
      "-2.8199999999999923\n",
      "-6.979999999999985\n"
     ]
    }
   ],
   "source": [
    "# In the Deepmind paper they use RMSProp however then Adam optimizer\n",
    "# improves training time\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 50000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 100000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 4\n",
    "# How often to update the target network\n",
    "update_target_network = 10000\n",
    "# Using huber loss for stability\n",
    "loss_function = keras.losses.Huber()\n",
    "\n",
    "while True:  # Run until solved\n",
    "    state = np.array(env.reset()[0])\n",
    "    episode_reward = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        # env.render(); Adding this line would show the attempts\n",
    "        # of the agent in a pop up window.\n",
    "        frame_count += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            # Take best action\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "\n",
    "\n",
    "        #plt.imshow(state_next, interpolation='nearest')\n",
    "        #plt.show()\n",
    "        #print(reward)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            #print(state_sample)\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices]\n",
    "            )\n",
    "\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            future_rewards = model_target.predict(state_next_sample)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
    "                future_rewards, axis=1\n",
    "            )\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Train the model on the states and updated Q-values\n",
    "                q_values = model(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        if done:\n",
    "            print(rewards_history[-1])\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "    episode_count += 1\n",
    "\n",
    "    if episode_count % 50 == 0:\n",
    "        #print('save model')\n",
    "        model.save('train_model')\n",
    "        model_target.save('target_model')\n",
    "\n",
    "    if running_reward > 10000000:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1fbb94b493f4a2be40eeededea278266a31d2929b69a0d694da97e233e8112ca"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('DA_Enviroment': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
