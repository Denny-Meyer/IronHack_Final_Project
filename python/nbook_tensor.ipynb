{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "## !pip install -e ../gym_space_docking/\n",
    "\n",
    "loc = os.popen('pip3 show gym_space_docking').readlines()[7].split()[1]\n",
    "\n",
    "\n",
    "sys.path.append(loc)\n",
    "\n",
    "\n",
    "import gym\n",
    "import os\n",
    "import gym_space_docking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include Network\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.9.6)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "/Users/denny/Documents/workspace_ironhack/IronHack_Final_Project/gym_space_docking/gym_space_docking/envs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]]], dtype=uint8),\n",
       " -0.02,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('space_docking-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "map, reward, done, info = env.step(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map.reshape(88,80,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(map, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n\n",
    "\n",
    "\n",
    "def create_q_model():\n",
    "    # Network defined by the Deepmind paper\n",
    "    inputs = layers.Input(shape=(88, 80, 3,))\n",
    "\n",
    "    # Convolutions on the frames on the screen\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "\n",
    "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-25 11:25:51.428788: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('train_model')\n",
    "model_target = keras.models.load_model('target_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = create_q_model()\n",
    "#model_target = create_q_model()\n",
    "\n",
    "#model.compile()\n",
    "#model_target.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration paramaters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon = 1.0  # Epsilon greedy parameter\n",
    "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ")  # Rate at which to reduce chance of random action being taken\n",
    "batch_size = 32  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-25 11:25:53.156044: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.279999999999986\n",
      "-5.879999999995654\n",
      "3.5799999999994228\n",
      "-37.10000000000162\n",
      "-24.579999999992207\n",
      "-4.639999999994139\n",
      "-0.8599999999999999\n",
      "-0.7600000000000005\n",
      "-12.179999999985728\n",
      "-1.9000000000000006\n",
      "-24.159999999987956\n",
      "-6.940000000000083\n",
      "-7.999999999999958\n",
      "-1.7600000000000005\n",
      "-1.1200000000000008\n",
      "-1.5199999999999991\n",
      "-1.139999999999998\n",
      "-22.219999999986438\n",
      "-49.33999999996961\n",
      "-12.459999999993752\n",
      "running reward: 121266.57 at episode 20, frame count 10000\n",
      "-30.799999999978272\n",
      "5.699999999999809\n",
      "-1.4399999999999937\n",
      "-1.8200000000000005\n",
      "-0.9799999999966835\n",
      "-33.619999999968485\n",
      "-4.159999999999965\n",
      "-4.460000000000031\n",
      "-2.4599999999999724\n",
      "-6.619999999999972\n",
      "-33.25999999998401\n",
      "-2.4199999999999964\n",
      "-1.0000000000000009\n",
      "-11.699999999996962\n",
      "1927.3200000000138\n",
      "-0.9200000000000008\n",
      "-17.51999999999171\n",
      "running reward: 120956.58 at episode 37, frame count 20000\n",
      "1093.8000000000168\n",
      "-21.019999999983558\n",
      "-21.259999999997103\n",
      "-9.719999999999962\n",
      "-3.679999999999983\n",
      "-2.9799999999999986\n",
      "-3.5199999999999827\n",
      "-4.359999999999963\n",
      "-3.0799999999999823\n",
      "-3.8199999999999736\n",
      "-0.7000000000000004\n",
      "-12.539999999995915\n",
      "-2.8199999999999954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-25 11:45:46.824806: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: train_model/assets\n",
      "INFO:tensorflow:Assets written to: target_model/assets\n",
      "-16.19999999999975\n",
      "-4.039999999999984\n",
      "-8.699999999989553\n",
      "-18.239999999998275\n",
      "-13.47999999999257\n",
      "-6.059999999999828\n",
      "-0.7400000000000007\n",
      "-6.679999999989951\n",
      "1290.0400000000063\n",
      "-3.0799999999999894\n",
      "-1.1000000000000008\n",
      "-3.039999999999968\n",
      "running reward: 110580.63 at episode 62, frame count 30000\n",
      "-51.059999999965676\n",
      "-2.2599999999999927\n",
      "-49.0999999999762\n",
      "-3.0199999999999934\n",
      "-2.3200000000000007\n",
      "-29.499999999986986\n",
      "1326.1400000000078\n",
      "-22.559999999994176\n",
      "1197.6600000000062\n",
      "-4.439999999999964\n",
      "-48.519999999967396\n",
      "-3.819999999999993\n",
      "-2.419999999999979\n",
      "1073.180000000017\n",
      "-1.200000000000001\n",
      "running reward: 146809.68 at episode 77, frame count 40000\n",
      "3.9599999999994324\n",
      "-2.6399999999999952\n",
      "-1.9199999999999982\n",
      "-3.9399999999999764\n",
      "-49.29999999997834\n",
      "-3.8999999999999932\n",
      "-24.659999999973195\n",
      "-0.9600000000000006\n",
      "-0.6400000000000003\n",
      "-12.199999999992627\n",
      "-1.7799999999999905\n",
      "-1.6800000000000006\n",
      "1598.660000000012\n",
      "-2.9199999999999893\n",
      "-34.319999999977654\n",
      "-4.099999999999974\n",
      "-1.8199999999999994\n",
      "-1.8200000000000005\n",
      "6.199999999999946\n",
      "1597.1200000000217\n",
      "running reward: 156169.25 at episode 97, frame count 50000\n",
      "-18.459999999982927\n",
      "-3.6399999999999864\n",
      "-13.839999999990646\n",
      "INFO:tensorflow:Assets written to: train_model/assets\n",
      "INFO:tensorflow:Assets written to: target_model/assets\n",
      "-5.27999999999996\n",
      "-11.679999999988636\n",
      "-6.1199999999999255\n",
      "-21.5399999999904\n",
      "-2.639999999999996\n",
      "-2.359999999999996\n",
      "-32.93999999999599\n",
      "-4.619999999999976\n",
      "-6.059999999999975\n",
      "-4.219999999999989\n",
      "-2.7999999999999963\n",
      "1508.240000000017\n",
      "4.819999999999727\n",
      "-0.7400000000000004\n",
      "-2.9999999999999867\n",
      "-16.859999999994198\n",
      "-23.880000000004692\n",
      "1117.9800000000193\n",
      "-6.139999999999978\n",
      "-9.439999999999912\n",
      "running reward: 159998.09 at episode 120, frame count 60000\n",
      "2027.1200000000067\n",
      "-1.9600000000000006\n",
      "-2.279999999999998\n",
      "-2.02\n",
      "-6.880000000000969\n",
      "-11.619999999986362\n",
      "-90.93999999994142\n",
      "-1.7400000000000004\n",
      "-2.6399999999999926\n",
      "-3.6799999999999864\n",
      "-9.59999999999985\n",
      "-2.6399999999999997\n",
      "1634.8400000000192\n",
      "-2.159999999999996\n",
      "-2.2799999999999923\n",
      "-7.719999999999951\n",
      "-11.839999999999906\n",
      "-23.519999999988162\n",
      "-1.2000000000000008\n",
      "-4.899999999999991\n",
      "-1.8200000000000007\n",
      "0.21999999999793785\n",
      "-3.4199999999999733\n",
      "-5.059999999999982\n",
      "-3.619999999999984\n",
      "-2.86\n",
      "running reward: 168318.33 at episode 146, frame count 70000\n",
      "-11.42000000000123\n",
      "-3.9399999999999826\n",
      "-3.5399999999999894\n",
      "-11.219999999999954\n",
      "INFO:tensorflow:Assets written to: train_model/assets\n",
      "INFO:tensorflow:Assets written to: target_model/assets\n",
      "-2.0399999999999996\n",
      "-4.37999999999998\n",
      "-2.9799999999999898\n",
      "-1.4399999999997588\n",
      "-3.039999999999995\n",
      "-7.799999999999949\n",
      "-2.0199999999999987\n",
      "-4.099999999999977\n",
      "1931.660000000034\n",
      "-14.55999999998603\n",
      "-1.1599999999999921\n",
      "-2.639999999999994\n",
      "-1.9800000000000004\n",
      "-2.6799999999999917\n",
      "-1.5400000000000003\n",
      "-7.239999999999876\n",
      "1238.94000000001\n",
      "-9.779999999999964\n",
      "1826.420000000008\n",
      "-1.2999999999999976\n",
      "-1.280000000000001\n",
      "-2.699999999999995\n",
      "-54.61999999997972\n",
      "running reward: 154785.81 at episode 173, frame count 80000\n",
      "1646.0000000000214\n",
      "-2.7599999999999967\n",
      "-4.659999999999963\n",
      "-2.459999999999998\n",
      "-2.539999999999978\n",
      "-1.439999999999999\n",
      "1196.3800000000058\n",
      "-1.7600000000000005\n",
      "-11.540000000000926\n",
      "-3.359999999999979\n",
      "0.9800000000048439\n",
      "-9.23999999999989\n",
      "-17.32000000000005\n",
      "-6.82000000000119\n",
      "-8.019999999999635\n",
      "-5.319999999999951\n",
      "-5.560000000000052\n",
      "-33.05999999998053\n",
      "-27.25999999999977\n",
      "-1.1799999999999864\n",
      "-3.45999999999999\n",
      "-37.87999999999882\n",
      "-5.540000000000012\n",
      "-38.679999999989434\n",
      "-2.6400000000000006\n",
      "-3.17999999999999\n",
      "1042.0800000000102\n",
      "INFO:tensorflow:Assets written to: train_model/assets\n",
      "INFO:tensorflow:Assets written to: target_model/assets\n",
      "running reward: 127540.73 at episode 200, frame count 90000\n",
      "-13.040000000001632\n",
      "-19.279999999995287\n",
      "-1.9799999999999964\n",
      "-32.45999999999887\n",
      "-1.6999999999999975\n",
      "-1.7799999999999985\n",
      "-4.919999999999973\n",
      "-7.979999999991681\n",
      "-1.1399999999999997\n",
      "-69.83999999999064\n",
      "-1.3599999999999977\n",
      "-2.6599999999999957\n",
      "-5.539999999999951\n",
      "-4.439999999999984\n",
      "-10.219999999999883\n",
      "-85.35999999996265\n",
      "-4.399999999999987\n",
      "-2.939999999999996\n",
      "-3.2999999999999945\n",
      "running reward: 116257.06 at episode 219, frame count 100000\n",
      "1587.6600000000392\n",
      "-1.0800000000000007\n",
      "-0.9400000000000006\n",
      "-3.459999999999991\n",
      "-1.7800000000015277\n",
      "-11.979999999999881\n",
      "-1.5399999999999965\n",
      "-0.8400000000000005\n",
      "-51.97999999999023\n",
      "-3.159999999999995\n",
      "-5.679999999999969\n",
      "-21.67999999998996\n",
      "-2.739999999999979\n",
      "-1.6400000000000006\n",
      "-14.87999999999992\n",
      "945.7000000000132\n",
      "-4.319999999999984\n",
      "-2.2199999999999864\n",
      "-0.6400000000000003\n",
      "-4.699999999999965\n",
      "-34.89999999997898\n",
      "1929.620000000011\n",
      "running reward: 113613.43 at episode 241, frame count 110000\n",
      "-22.459999999994807\n",
      "-10.219999999999937\n",
      "-23.25999999998995\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/58/83__n5jj77s0p9pv81ccxck40000gn/T/ipykernel_1793/617093800.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;31m# Build the updated Q-values for the sampled future states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# Use the target model for stability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mfuture_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_next_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0;31m# Q value = reward + discount factor * expected future reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
      "\u001b[0;32m/opt/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1618\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[0;32m/opt/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    680\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    703\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 705\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m       \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[0;32m/opt/anaconda3/envs/DA_Enviroment/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   2969\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2970\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2971\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   2972\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   2973\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# In the Deepmind paper they use RMSProp however then Adam optimizer\n",
    "# improves training time\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 50000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 100000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 4\n",
    "# How often to update the target network\n",
    "update_target_network = 10000\n",
    "# Using huber loss for stability\n",
    "loss_function = keras.losses.Huber()\n",
    "\n",
    "while True:  # Run until solved\n",
    "    state = np.array(env.reset()[0])\n",
    "    episode_reward = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        # env.render(); Adding this line would show the attempts\n",
    "        # of the agent in a pop up window.\n",
    "        frame_count += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            # Take best action\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "\n",
    "\n",
    "        #plt.imshow(state_next, interpolation='nearest')\n",
    "        #plt.show()\n",
    "        #print(reward)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            #print(state_sample)\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices]\n",
    "            )\n",
    "\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            future_rewards = model_target.predict(state_next_sample)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
    "                future_rewards, axis=1\n",
    "            )\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Train the model on the states and updated Q-values\n",
    "                q_values = model(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        if done:\n",
    "            print(rewards_history[-1])\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "    episode_count += 1\n",
    "\n",
    "    if episode_count % 50 == 0:\n",
    "        #print('save model')\n",
    "        model.save('train_model')\n",
    "        model_target.save('target_model')\n",
    "\n",
    "    if running_reward > 10000000:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1fbb94b493f4a2be40eeededea278266a31d2929b69a0d694da97e233e8112ca"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('DA_Enviroment': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
